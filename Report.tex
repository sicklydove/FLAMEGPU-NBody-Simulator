%Dissertation writeup%

%Housekeeping
\documentclass[11pt,a4paper]{article}
\usepackage{hyperref}
\usepackage{sidecap}
\usepackage{algorithm}
\usepackage{float}
\usepackage{graphicx}
\usepackage{url}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{a4wide}
\usepackage{algorithmic}
\usepackage{wrapfig}
\usepackage[small]{caption}
\usepackage{fullpage}
\usepackage{listings}

\begin{document}
\pagenumbering{roman}

%formatting
\numberwithin{equation}{section}
\setcounter{tocdepth}{3} 

%Title page
\input{./titlePage.tex}

%Statement
\newpage 
\noindent All sentences or passages quoted in this report from other people's work have been specifically acknowledged by clear cross-referencing to author, work and page(s) (where possible). Any illustrations which are not the work of the author of this report have been used with the explicit permission of the originator and are specifically acknowledged. I understand that failure to do this amounts to plagiarism and will be considered grounds for failure in this project and the degree examination as a whole. 
\\[0.3cm]
\large{Name: Laurence James}
\\[0.3cm]
\large{Signature:}
\\[0.3cm]
\large{Date: \today}

\newpage
\begin{center}
\bf{Abstract}
\end{center}
As hardware has become increasingly powerful, the doors have been opened for a wide range of more computationally intensive simulation procedures. In particular, agent-based modelling has seen a recent surge of interest in the fields of Biology and Economics. For this project we propose using an agent-based model to create an implementation of the classic physics problem of cosmological N-body simulation, in order to investigate the process of galaxy formation. Our simulation should be displayed as an evolving 3D graphic, to allow for easy interpretation of the data.

The GPU, with its massively-parallel architecture is well suited to the computation of N-body systems, and allows us to create high-fidelity simulations using mid-level consumer hardware. We therefore also investigate methods for performing our model's calculations on the GPU.

The primary aims of the project are to determine whether agent-based models are suitable for the investigation of physical phenomena, and to produce a realistic model of galaxy formation in doing so.
%\newpage
%\begin{center}
%\bf{Acknowledgements}
%\end{center}
%A number of people deserve my thanks for their support and assistance throughout the development of this project. They are:
%\\[0.3cm]
%Firstly, my supervisor Dr Mike Stannett, for suggesting the project, answering my queries and reviewing countless drafts.
%\\[0.3cm]
%Simon Coakley and Paul Richardson, for their work on, and assistance with, FLAME and FLAME GPU, respectively.
%\\[0.3cm]
%And of course, my family and friends, without whom I wouldn't have finished even the first semester of the degree, let alone a project such as this. In particular, my parents, Sophie, and the CiP HoN brews (top 4 UK).
\newpage

\tableofcontents
\listoffigures

\small
%Introduction
\newpage
\pagenumbering{arabic}
\begin{center}
\section{Introduction}
\end{center}

From Aristarchus of Samos\footnote{Aristarchus was the first to propose a helio-centric model of the Solar-System; \url{www.astro.cornell.edu/academics/courses/atro201/aristarchus.htm}} to Fritz Zwicky\footnote{Zwicky first realised a large proportion of matter must be ``unseen''\cite{Ty01} (now referred to as `Dark Matter')}, humans have been watching and analysing the movement of the stars.
As science and technology progress we use ever more complex instruments to investigate the universe, and the resulting theories become increasingly precise. Recently, our models of how the universe formed under the influence of physical laws have been rigorously tested with a barrage of new data. Results from the \emph{COBE}\footnote{COsmic Background Explorer; \url{http://lambda.gsfc.nasa.gov/produce/cobe/}} and, more recently, \emph{WMAP}\footnote{Wilkinson Microwave Anisotropy Probe; \url{http://lambda.gsfc.nasa.gov/produce/map/current}} showed us that the Cosmic Microwave Background Radiation\footnote{Radiation `left over' from the recombination phase of the early universe.} (CMBR) isn't isotropic; we find tiny fluctuations in the levels of radiation (figure \ref{cmbrImg}). These fluctuations comprise some of the best evidence we have for the idea that large-scale structures such as globular clusters, galaxies and galaxy clusters formed bottom-up around a small `seed' of slightly higher matter density in the very early universe. This data led to a surge of interest and a revamping of our models, which are now being further tested with supercomputer simulations. 

In Isaac Newton's \emph{Principia}, he laid out the famous Laws of Classical Motion, from which the \emph{Universal Law of Gravitational Attraction}, as well as Kepler's laws of planetary motion (which previously had only been shown empirically) he was able to derive. The relatively simple law of gravitational attraction is amongst the most profoundly important of classical physics; in words, it states that two masses are attracted towards each other across \emph{any distance} with a force proportional to the product of their masses, and inversely proportional to the square of their distance. The constant of proportionality, $G$, is termed the \emph{Gravitational Constant}, and $G\approx6.674\times 10^{-11}$N(m/kg)$^2$. Within a programming context, this is just a fixed global constant `magic number'.

Comparing the force exerted by gravity to that of other fundamental forces (electromagnetism, for example), we find that gravity is, in fact, incredibly weak; if we take two electrons then the ratio of (attraction due to gravity):(repulsion due to electromagnetism) is given as $1.41\times 10^{-42}$\cite{Fe64}. This is at the microscopic scale, however. On the scale at which we live, our experience of the world is dominated by the effects of gravity doing what it does best --- acting on a macroscopic level. In the universe, large collections of matter are electromagnetically neutral, so this force doesn't dominate their structure. Gravity however, is less discerning; it affects all matter, and its infinite range allows it to slowly pull objects across vast distances into the structures we see now.

We observe gravitationally bound systems in vastly different scales; from tiny satellites orbiting asteroids to galaxy clusters (superclusters are not gravitationally bound systems; the distances between their component clusters are much too vast). Those which are most relevant to the project include star groups/clusters, galaxies, and galaxy groups/clusters.

The formation of these large-scale structures, the expansion of the universe and the existence of the Cosmic-Microwave-Background radiation noted earlier is explained by the $\Lambda$-CDM\footnote{Lambda-Cold Dark Matter; See section \ref{lcdm}} model, which this study aims to investigate with reference to the formation of Galaxies. To do this, we will be simulating a universe of particles under the effect of physical laws. For the sake of simplicity we will be working with a finite, 3D universe of Euclidean geometry, and so $\Lambda$ --- the Cosmological Constant (the `dark energy' which controls the universe's rate of expansion) --- is not relevant.
 
Structure formation has been already been extensively tested via simulation. Here, we populate a `universe' with a set of particles and then compute the effects of the physical laws (gravity, electromagnetism, etc\ldots) on these particles, and watch as the system evolves. Simulations of this nature are known as \emph{Cosmological N-body} simulations, and are usually solved directly by integration on the systems of differential equations representing the particles' movement under the effect the relevant forces (usually just gravity; section \ref{similarStudies}). As such, the aim of this project is not merely to replicate preexisting studies with less time, resources and knowledge, but to use a technique previously unused in this field --- \emph{Agent-Based Modelling} (ABM) --- where particles (in the N-body sense; we're not simulating individual particles of matter) are represented as individual autonomous \emph{agents}. ABM is well suited to the simulation of systems where the location of agents is important, and where large systems are composed of many simpler ones \cite{abmPresentation}; phenomena which we see in galaxies built of many star systems and globular clusters, as well as galaxy clusters comprising many mutually bound galaxies. 

A final advantage of ABM is that it allows us to use unique agents at separate levels of abstraction in the same system. We therefore have the potential to have a set of agents of stars, some number of agents as gas clouds, others representing black holes, and so on, each with different rules rules governing their interaction with other objects.
 
\begin{figure}[h!]
\centering
\includegraphics[scale=0.6]{./images/wmap}
\caption[Cosmic-Microwave-Background Radiation map]{The Cosmic-Microwave-Background Radiation, showing the very slight temperature anisotropies in the early universe. The widest range of temperatures is $\pm 200\times10^{-6}$ Kelvin \cite{nasaWMAP}}
\label{cmbrImg}
\end{figure}

%Literature Review - let's go!
\section{Literature Review}

\subsection{Overview}
Since this project spans multiple fields (modelling/simulation, computer science and physics), this literature review aims to cover the material relevant to each in a logical and structured manner. First, we cover information relating to the cosmological aspects of the project, mainly with reference to structure formation. Next, an explanation of some astronomy and the various smaller-scale astrophysical phenomena likely to have a bearing on the implementation. We then look at the aspects more directly related to computation; discussing Newtonian gravity specifically with reference to the calculations required for N-body simulations, and the different algorithms we can use for this. Next, we discuss existing tools and modelling frameworks likely to aid the project's implementation, before finally looking at previous studies which have carried out similar investigations.

\subsection{The Early Universe}
\label{earlyUniverse}
In order to fully grasp the background and extent of the project, it is important to have a basic understanding of the evolution of the universe as a whole, as described by the $\Lambda$-CDM `standard model'. Not only does this provide us with a good mental framework from which to visualise what's going on in the simulation, but we can also appreciate the extent to which we have to simplify the physics in order to make this project feasible given the time constraints.

At the very beginning of time immediately after the Big Bang, we enter what is known as the Planck epoch, followed immediately by the grand unification and electroweak epochs. Not particularly relevant to this project, but of great interest to theoretical physicists, these are the time periods where, if our models are correct, the fundamental forces were combined; gravity broke away at the start of the grand unification, and the strong nuclear force broke away at the start of the electroweak epochs, respectively. 

The inflationary epoch is the first with importance to structure formation. During this period space expanded rapidly --- much faster than the speed of light. It is thought that during this period, quantum fluctuations `froze in' \cite{QC} and became \emph{primordial fluctuations}; the density variations which we now (indirectly) observe as the anisotropies in the CMBMap results from COBE and WMAP (figure \ref{cmbrImg}).

With the inflationary period concluded at $t\approx10^{-32}$s, the universe is filled with an extremely high temperature \emph{quark-gluon plasma} (``quark soup'') --- an extremely regular distribution of high-energy quarks\footnote{Quarks are elementary particles, and form hadrons when two or three are held together by the strong nuclear force.} and gluons\footnote{Gluons are exchange particles which mediate the strong nuclear force.}. Over the following twenty minutes, the universe undergoes various changes responsible for the relative distributions of particles we see today; the plasma cools, and hadrons\footnote{Formed from quarks, hadrons are split into baryons (including protons and neutrons), and mesons, which are extremely unstable.} form. During this period nucleosynthesis begins, and Hydrogen and Helium-4 nuclei\footnote{Protons and alpha particles, respectively.} are produced as the protons and neutrons undergo nuclear fusion. 

This period is extremely important for understanding where to start the model. With further cooling of the universe, cold dark matter around the primordial fluctuations provides a gravitational seed for matter density to increase around. However, for about $377,000$ years after nucleosynthesis the universe still only contains ionised plasma. It is only here, at recombination, that hydrogen and helium \emph{atoms} start to form as free electrons are caught by their nuclei.

With the presence of Hydrogen and Helium gas, as well as small areas of increased density, we are finally in a position to observe structure formation. The amplitude of density perturbations grows as small quantities of gas are added until their rate of growth becomes non-linear, at which point gravitationally bound groups grow at an increasing rate via mergers due to collisions with others \cite{recipe}. It is important to note that the dark matter around which these formed coalesces also, and remains around these objects as a \emph{dark matter halo} \cite{radial}. As these clouds increase in mass and energy, the first stars form within them. Over time, as more mass is accrued and more stars form, these objects become the first dwarf and proto-galaxies. With further collisions and mergers between dwarf galaxies the system's mass and energy continue to increase, as star-formation continues in giant clouds of hydrogen and helium. Eventually, the first galaxies are formed, now observed by us as the very distant \emph{quasars}. 

\subsection{$\Lambda$-CDM}
\label{lcdm}
The `Standard Model' of Cosmology is known as $\Lambda$-CDM. Assuming time and space were created by a `Big Bang' event, the $\Lambda$-CDM attempts to explain how the universe transitioned from this initial hot, dense, practically isotropic state through to the complex, structured Universe which we now observe. The model of structure formation was roughly explained in section \ref{earlyUniverse}, and so this section is more concerned with the assumptions and requirements of the $\Lambda$-CDM which are directly relevant to our simulation.

The mathematics of the $\Lambda$-CDM (the FLRW metric, Friedmann equations and cosmological equations of state) require a set of parameters, some of which will be incorporated into our simulation. These are: 

\begin{figure}[H]
\centering
\includegraphics[scale=0.6]{./images/LCDMParameters}
\caption[Table of parameters for the $\Lambda$-CDM]{Table of $\Lambda$-CDM parameters estimated from the most recent WMAP readings, taken from `The Cosmological Parameters' Page 5 \cite{param}}
\label{cosmologicalParameters}
\end{figure}

\noindent Thankfully, our simulation simplifies most of these away. Those which will be relevant are the baryon density (the quantity of baryons in the universe); the dark matter density (the quantity of dark matter), and the density perturbation amplitude, which when used in conjunction with a Gaussian random field, should give an appropriate early-universe matter distribution.

\subsubsection{$\Lambda$ --- The Cosmological constant \& dark energy}
First included in Einstein's `Field equations for General Relativity'\cite{einsteinField}, the \emph{Cosmological constant} controls the rate of the expansion of space. The $\Lambda$-CDM model includes the concept of \emph{dark-energy} --- a physical realisation of the cosmological constant. This dark energy drives the expansion of the universe, and so every point becomes further away from every other point. This becomes quite an important simplification for our project --- if we wish to deal with a fixed-space universe, we must ignore the universe's Dark Energy component. Many previous N-body simulations have modelled in this manner, and GADGET (section \ref{gadget2}) includes options to use non-expanding universes.

This simplification helps us in a number of ways. Firstly, the mathematics dealing with particles' positions at each timestep is much simpler; we don't need to alter them to account for the universe's expansion. Secondly, the expansion would act in opposition to gravity --- as gravity brings objects together, the expansion pulls them apart. In this case, much care would need to be taken in balancing these phenomena to still see structure formation and avoid universes dominated by entropy.

\subsubsection{CDM -- Cold Dark Matter}
When we look at the rotation curves of galaxies we find that their rotational velocity does not decrease with distance from the centre as it should, given the galaxy's apparent mass distribution. To fix this discrepancy, additional mass is required in the form of a `halo' around the galaxy \cite{radial}. Since we can't observe this matter interacting with electromagnetic radiation we term it \emph{dark}.

The variant of dark matter in the $\Lambda$-CDM is believed to be cold, that is, moving at slow (non-relativistic) speeds, and accounts for $23\%$ of the mass-energy\footnote{According to the mass-energy equivalence, energy can be expressed as a function of mass, and visa-versa. By using mass-energy, we normalise everything to one unit.} in the Universe, or $83\%$ of the matter in the universe (ordinary `baryonic'\footnote{Baryonic matter is that which is made up predominantly of baryons; particles built from three quarks.} matter making up for the remaining $17\%$). 

Unlike dark energy, dark matter is vital to our simulation --- essentially providing a gravitational frame around which structures such as dwarf and proto-galaxies can form. Indeed, the Millennium simulation (section \ref{MilSim}) didn't model baryonic matter at all --- it investigated structure formation on such a large scale ($\approx 2\times 10^6$ galaxies), that \emph{only} dark matter particles were used \cite{millenium}.

\subsection{Stars}
\subsubsection{Star formation}
If we want to simulate realistic galaxies, we're going to want them to contain stars. If we want our simulation to include stars, and we don't want to write them directly into the $t_{0}$ file\footnote{When modelling a system like this we specify some initial conditions for the simulation, these constitute the $t_0$ file (section \ref{flamet0}).}, we need some method of creating stars from agents which represent gas or molecular clouds\footnote{A molecular cloud is an interstellar cloud (a collection of gas, plasma and dust), whose density is high enough to permit the formation of molecules. Stars are believed to form from the collapse of these.}. 

Now, the physics of star formation are easily complex enough to make another entire simulation project out of, so we need to implement some simplifications. Generally, the process of star formation begins with the collapse of a gas cloud whose gravitational potential energy is greater in magnitude than its kinetic energy. That is, the force acting inward due to gravity is greater than the outward force of pressure.

\label{jean}
To find if a cloud is going to undergo gravitational collapse, we check that it's mass is greater than the limit set out by the Jean's criterion \cite{jean}:

\begin{equation}
M_{cloud} > (\frac{5kT}{Gm})^\frac{3}{2}(\frac{3}{4\pi\rho})^\frac{1}{2}
\end{equation}

\noindent with $M_{cloud}$ the cloud's mass; $m$, the mass per particle; $k$ the Boltzmann constant; $T$ the average temperature of atoms in the cloud; $G$ the gravitational constant; and $\rho$ the cloud's density.

Thankfully, all these values should be relatively simple to calculate or approximate. If we work with agents representing gas clouds, their masses can be instantiated in $t_{0}$ (or calculated as the sum of their component `particle' agents) and recalculated appropriately as a result of mergers. Similarly $\rho$ we can assume to be uniform, then calculate $\rho=\frac{m}{V}$, with volume also instantiated to some realistic value at $t_{0}$. Finally, $T$ can be estimated from the velocities of particles within the cloud.

\subsubsection{Supernovae \& Black Holes}
Including supernovae and black holes (the final stage of evolution for particularly massive stars) could be interesting. When a sufficiently massive star ($M_{star}\geq 9$ solar masses\footnote{Our sun is one solar mass.} \cite{superstar}) exhausts the supply of elements which it can fuse for energy, it collapses. If the remaining core exceeds about 1.44 solar masses (the `Chandrasekhar limit'), it implodes. This implosion causes a huge shock wave which could affect the simulation in a number of ways; possibly by `blowing out' nearby gas clouds, or by inducing high levels of star formation in gas-rich areas. It should also prove relatively simple to display them as a particularly bright/high-energy source in our visual representation.

Black holes can play an important part in galaxy structure --- it has recently been confirmed that our Milky Way contains a particularly large `supermassive' black hole at its centre, and that this is likely the case for most, if not all, others \cite{milkyway, blackHoles}.
Given that our system ignores relativistic effects (section \ref{newtonGravity}), the particular physics of black holes aren't necessarily relevant. The formation of galaxies with or around extremely large, central masses would therefore add additional weight to the reliability of our model. 

\subsection{Galaxy Types}
We categorise galaxies according to the \emph{Hubble Sequence} (known colloquially as the `Tuning Fork', on account of its shape). The tuning fork sorts galaxies into four types, outlined here.
\begin{figure}[h!]
\centering
\includegraphics[scale=0.6]{./images/tuningfork}
\caption[The Hubble sequence diagram]{The Hubble sequence diagram \cite{tuningFork}.}
\label{tuningForkDiag}
\end{figure}

\subsubsection{Elliptical galaxies}
Elliptical galaxies, located along the centre (the handle, if we're using the tuning fork analogy), are roughly spherical or ellipsoidal; essentially giant balls of stars and gas. Currently, we believe that elliptical galaxies are likely to be the result of collisions or mergers between other galaxies, where strong tidal forces disrupt the spiral. This theory is supported by other properties of elliptical galaxies; namely that the stars in them are much older, and that they appear to be less common in the early universe, which we observe with the Hubble deep field\footnote{The Hubble deep field is a composite image of a section of sky, taken with an extremely long exposure. The objects observed in the Hubble deep field are incredibly distant, and therefore existed when the universe was very young.} \cite{deepEliptical}. Additional support is given by the facts that we see them more often among dense galaxy clusters, and that ellipticals comprise the majority of the largest known galaxies (the huge ellipticals being the result of multiple galaxy collisions). Despite this however, we do see a great range of sizes in ellipticals --- indeed, with most dwarf galaxies also classifying as such. \cite{sloan}

 
\subsubsection{Spiral galaxies}
Positioned on the two branches on the right of the diagram, spiral galaxies are extremely flat (relative to width) disks, with their matter rotating around a dense central bulge of older matter and stars, usually surrounding a supermassive black hole. We split spirals galaxies into two classes: (regular) spirals, and \emph{barred} spirals. Barred spiral galaxies (the diagram's lower branch) contain a central bar of many stars.

Spiral galaxies take their name from the long `arms' of matter we observe extending from and rotating around them, thus forming the distinctive structure. The spiral arms of different galaxies are `wound' in varying degrees of tightness, from long and spread out (the most extreme cases found in barred spirals; SBc in figure \ref{tuningForkDiag}) to those with many, tightly wound arms, such as the pinwheel galaxy (appendix \ref{pinwheel}). The arms contain much higher levels of new star formation than other areas of galaxy, and so have a distinctive blue hue.

The majority of galaxies observed in our universe are spirals \cite{morphology} (our own Milky Way is believed to be a barred spiral). As such, if by the conclusion of this project we are able to observe the formation of a reasonably standard spiral galaxy, we could certainly consider this at least a partial success.

\subsubsection{Lenticular galaxies}
At the centre of the fork ($S0$), lenticular galaxies provide a bridging point between the elliptical and spiral classes. Like spiral galaxies, they're relatively flat disks with a central bulge. Like elliptical galaxies on the other hand, they don't appear to have any spiral arms, and there is very little active star formation --- the majority of stars are old and dim --- giving the galaxy a faintly reddish hue more akin to that of the ellipticals than the blue seen in spiral arms.

Lenticular galaxies are fairly uncommon \cite{morphology}, and probably won't have much impact on our simulation. The main theory for their formation, as one might expect, is that they are simply spiral galaxies which have grown old and faded, with shock waves blowing out the interstellar matter used for star formation \cite{tully}. This is somewhat contested by the notion that the bulges are too large and the galaxies too bright for this to be the case, and so they must have formed via mergers \cite{lentform}. 

Whatever the outcome, both theories require us to accurately model the galaxy's evolution across long periods of time, and so we probably won't see any in the simulation; we're investigating the \emph{formation} of galaxies, not the evolution of those which are extremely old.

\subsubsection{Irregular galaxies}
Included later as a catch-all for those galaxies which defy classification to any of the above three sections, irregular galaxies are rare and come in all shapes and sizes. These are generally thought to be the result of spiral or elliptical galaxies distorted by the gravitational pull of a near-neighbour. We may find that if our mathematical simplifications are too great, or our simulation's resolution is too low, then all galaxies simply appear irregular, with no governing spin or overarching structure.

\subsection{Newtonian Gravity}
\label{newtonGravity}

Although technically superseded by Einstein's \emph{General Theory of Relativity}, we can assume Newtonian gravity to be appropriate for use in our model as the mechanism behind which structure formation takes place. This assumption is used based on the success of a significant number of non-relativistic cosmological N-body simulations. Newtonian gravity will therefore govern the motion of our objects as they move through space, and so we need some accurate method of calculating this.

\subsubsection{Formulae and calculations} 

For objects simplified to point-masses, Newton's Universal Law of Gravitational Attraction is given by:

\begin{center}
\begin{equation}
F=\frac{Gm_{1}m_{2}}{r^2} 
\label{gmmr2}
\end{equation}
\end{center}

\noindent with $F$ the force, $G$ the Gravitational Constant, $m_{n}$ the mass of body $n$, and $r$ the distance between the two objects. 

\noindent Substituting Newton's second law, $F=ma$, into \ref{gmmr2}, a body of mass $m_{1}$ accelerates towards a body of mass $m_{2}$ with acceleration:
\begin{center}
\begin{equation}
a=\frac{Gm_{1}m_{2}}{r^2m_{1}} = \frac{Gm_{2}}{r^2}
\label{substitudedAccn}
\end{equation}
\end{center}

Equation \noindent \ref{substitudedAccn} therefore gives the acceleration due to gravity on some body from an object. When resolving the resultant force on a body, $i$ for any number of $j$ objects, we simply take the vector sum of all forces. Generalising \ref{substitudedAccn} to many objects in three dimensions, we get:

\begin{center}
\begin{equation}
\mathbf{\ddot r_{i}}=G\sum_{i\neq j}\frac{m_{j}(\mathbf r_{j}-\mathbf r_{i})}{|\mathbf r_{j}-\mathbf r_{i}|^3}
\label{accnFinal}
\end{equation}
\end{center}

\noindent with $\mathbf {\ddot r_{i}}$ the acceleration of object $i$, and $\mathbf r_{n}$ the position in space of object $n$ as a 3D Vector
\\[0.3cm]
\noindent From basic classical mechanics we know that
$\int \! \mathbf a(t) \, \mathrm{d}t = \mathbf v(t)$ and 
$\int \! \mathbf v(t) \, \mathrm{d}t = \mathbf r(t)$
\noindent (with $\mathbf a(t)$ the acceleration at time $t$, $\mathbf v(t)$ the velocity at time $t$, and $\mathbf r(t)$ the displacement at time $t$)

\noindent Expanding these integrals $\mathrm{d}t$ with some small timestep $\Delta t=t-t_0$ gives:
\begin{equation}
\mathbf v_i(t) = \mathbf a_i \Delta t + \mathbf v_i(t_0)
\label{velocityEq}
\end{equation}
\begin{equation}
\mathbf r_i(t)=\frac{1}{2}\mathbf a_i\Delta t^2 + \mathbf v_i(t_0)\Delta t + \mathbf r_i(t_0)
\label{displacementEq}
\end{equation}

\noindent with $\mathbf v_i$ the velocity of object $i$; $\mathbf a_i$ the acceleration of object $i$ as calculated by \ref{accnFinal}, and $(t_0)$ the previous timestep.


\subsubsection{The N-body problem}
\label{optimisation}
With $\Delta t$ sufficiently small, we can assume that the acceleration remains constant for this period. This assumption is of fundamental importance to simulations of this type; for each timestep we calculate the acceleration for every particle. This acceleration is then used in \ref{velocityEq} and \ref{displacementEq} to find the vector displacement for each particle at the new time $t=t_0+\Delta t$. The positions are then updated as a result of the displacement, and the iteration is concluded. We repeat this until some suitable number of timesteps has passed.

Any system which attempts to simulate the gravitational attraction between a set of bodies (usually with a method similar to that described above) is termed an \emph{N-body Simulation}. (Examples of previous N-body simulations include the \emph{Millennium run} and \emph{Bolshoi Simulation}, and are given in section \ref{similarStudies}.)

N-body simulations become problematic, however, as we generalise \ref{accnFinal} to find the resultant positions for \emph{all} points at \emph{all} times, given some initial positions and velocities. This problem stems from the fact that any N-body system becomes chaotic with $N>2$ \cite{chaoticNbody}. This is termed the \emph{$N$-body problem}, and is expressed mathematically as when we wish to solve \ref{accnFinal} for $i=1, \ldots, N$

%\begin{center}
%\begin{equation}
%\mathbf{\ddot r_{j}}=G\sum_{i\neq j}\frac{m_{i}(\mathbf r_{i}-\mathbf r_{j})}{|\mathbf r_{i}-\mathbf r_{j}|^3},i=1, \ldots, N 
%\end{equation}
%\end{center}

When performing N-body simulations rather than finding some general solution we opt to directly calculate the positions of each body (hereafter referred to as `particles'), for a number of discrete timesteps. With enough timesteps the particles' movements appear continuous. A number of algorithms exist for computing these particle movements, outlined below.

Information on Barnes-Hut, particle-mesh and $P^3$M approaches (including algorithms 2 and 3) taken from Lindholm's N-body algorithm seminar: \cite{nbodyAlg}.

\paragraph{All-pairs}\label{allPairs}(also `brute force', `pairwise' or `particle-particle') approaches are the most basic. As you might expect, with an all-pairs N-body computation algorithm we simply calculate \ref{accnFinal} for every particle in the system. That is, for every particle we compute its acceleration due to every other particle. This is obviously rather computationally expensive --- with $N$ particles, the algorithm has O($N^2$) complexity. This potentially restricts the number of particles we can model with, thus reducing the simulation's fidelity. 


\paragraph{Tree-based}\label{barnesHut}methods treat groups of extremely distant objects as point masses located at their combined centre of mass. With attraction decreasing as the square of the distance, it makes little sense to put the same effort into treating a star in some far-off galaxy the same as one which might be only a few light-years away. Instead, we treat the whole \emph{galaxy} the same as our hypothetical star, reducing it to only one body with its mass and position given as the total and centre of mass for all combined particles, respectively. 

To do this, we split space into cells in a tree structure, and so create an octree of our simulation space (quadtree when working with 2 dimensions) by recursively subdividing our volume. We continue until each particle is enclosed in a leaf node (figure \ref{msPaintTreeDiag}; more detailed version in appendix \ref{barnesHutImg}). 

\begin{figure}[h!]
\centering
\includegraphics[scale=0.80]{./images/tree}
\caption[Barnes-Hut subdivision of particle-space, and associated tree diagram]{Diagram showing initial particle space (particles as a,b,c\ldots); particle space once we've performed the subdividing, and the associated quadtree. Tree labels calculated top-left to bottom-right. Author's own work.}
\label{msPaintTreeDiag}
\end{figure}

With the tree constructed, we can generalise sets of particles to higher-level nodes. For example, in figure \ref{msPaintTreeDiag} when calculating the force for a particle such as $g$, we might combine all particles under $1.2$ to just one; giving some new particle, say $h$ with mass $M_h = M_b + M_c + M_d$, and position their centre of mass. 

Obviously, we can do this for any node based on our distance threshold. Particle $a$, for example, might use $e$ individually, but combine $f$ and $g$ to one. 
The set of particles `belonging' to a node is those which are at any depth below that node. So, the set of particles in 1.3.3 is a subset of those in 1.3; is a subset of those in 1.

The \emph{Barnes-Hut simulation} introduced the tree-based method described above, and optimally reduces complexity from O($N^2$) to O($N \log N$) \cite{barneshut}. It contains three steps:

\begin{enumerate}
\item Compute octree
\item Traverse from leaves to root, computing total mass and center of mass for each parent node
\item For each particle, traverse from root calculating the appropriate force 
\end{enumerate}

\begin{algorithm}[H]
\caption{Function to recursively subdivide cells until all particles on leaves. (As in fig \ref{msPaintTreeDiag}, appendix \ref{barnesHutImg})}

\begin{algorithmic}
\STATE split current cell into 8
\FORALL{new cells, $x$}
  \IF{x contains $>$1 particle}
    \STATE{Call this function recursively on $x$}
  \ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H] 
\caption{Function to compute step 2 of Barnes-Hut algorithm:}
\begin{algorithmic}
\IF{leaf node}
  \RETURN
\ELSE 
  \FORALL{children, $x$} 
  \STATE call this recursively on $x$
  \ENDFOR
\ENDIF
\STATE mass = 0
\STATE distance = 0
\FORALL{children, $x$}
 \STATE{mass += mass of $x$}
 \STATE distance += (mass of $x$ $\times$ position of $x$)
\ENDFOR
\STATE distance = distance/mass
\end{algorithmic}
\end{algorithm}

\noindent When calculating the force for step 3 we find the ratio, $\theta$, where $\theta = \frac{D}{r}$ (with $D$ the size of the current node and $r$ the distance to some other node $B$) If $\theta < $(some limit), we use $B$ as a generalisation for its children. Otherwise, we compare again with each of $B$'s children.

\begin{algorithm}[H]
\caption{Function to compute step 3 of Barnes-Hut algorithm:}
\begin{algorithmic}
\IF{(leaf node) OR (((size) / (distance(B))) $<\theta$)}
  \RETURN force between this and B
\ELSE
 \STATE force=0
 \FORALL{children, $x$}
   \STATE force += (this recursively on $x$)
 \ENDFOR
 \RETURN force
\ENDIF
\end{algorithmic}
\end{algorithm}

\paragraph{Particle-Mesh based optimisation:}While our previous descriptions use discrete timesteps to measure time, particle-mesh methods expand upon this idea by also measuring \emph{position} discretely.

We first split the universe into a grid (`mesh'), which gives us our fixed positions, and then solving \ref{poisson} at each meshpoint gives us the gravitational potential at that position:
\begin{equation}
\nabla^2\Phi=c\rho
\label{poisson}
\end{equation}

\noindent with $c$ a constant; $\Phi$ the gravitational potential energy and $\rho$ the mass density, we can now define some $U$ as the force, with: $\nabla \cdot U = c\rho$; 

For the purposes of calculating the density function, $\rho$, particle masses are assigned to cells by way of either the `nearest gridpoint', where each particle's entire mass is given to the closest grid position, or by `cloud-in-cell', where the particle's mass is distributed over the eight neighboring cells by a weighting function determined by the particle's proximity to each cell.

\paragraph{Particle-Particle/Particle-Mesh:}
The traditional particle-mesh algorithm is, however, a rather poor method\cite{nbodyAlg} for computing short-range interaction. Therefore, the \emph{Particle-Particle/Particle-Mesh} ($P^3M$) algorithm was devised. ($P^3M$) essentially serves as a compromise by using a PM-based algorithm for long-range forces, and pairwise comparison for short-range forces. When using $P^3M$ we first calculate the overall force on our particle with a standard PM method, and then subtract from this the total force given by objects within some radius around our particle. The forces for each particle within this radius are then individually calculated with pairwise comparison as described in section \ref{allPairs}, and summed to give the final result.

\subsection{Agent-models \& software packages}
\label{potentialSoftware}

The general concept behind agent-based modelling is to represent each individual actor in some situation as an autonomous \emph{agent}. Each agent is an instance of some agent type, and their behaviour is governed by a set of rules and variables defining that type. By then populating a system with a number of these agents, their low-level interplay prompts changes at the macroscopic level; this generates the system's \emph{emergent behaviour}. Furthermore, we can have higher-order agents defined as collections of individual, lower-level agents. The low-level agents will then still behave according to their own rules individually, but might have a different role in the context of some greater system. We find ABMs are becoming increasingly used in biology, economics and social sciences \cite{abmPresentation}; a key strength being that we only need to understand the behaviour of what we're simulating on an individual level. If the interaction of two cells is well understood, but the interaction of two-hundred thousand less so, then we only need to implement the low-level knowledge in our system and then observe the resultant behaviour.

By using an agent-based modelling package to control the actual execution of our simulation, we avoid having to invest a significant portion of time into creating bespoke software. This therefore allows us to focus all our efforts on producing a more accurate simulation, incorporating more phenomena or removing mathematical simplifications as appropriate.

Rob Allan's 2009 paper comparing various agent-based modelling and simulation tools \cite{robABM} outlined forty-three different packages, each with different approaches, capabilities and specialities. Unfortunately, there is very little general comparison available, and so choosing which package to use could prove quite a task. We look in detail at two of these; `Swarm' and `FLAME'\footnote{Flexible Large-scale Agent Modelling Environment; \url{www.flame.ac.uk}}. Swarm was the first generic software tool for creating and executing agent-based models, and is the most well-known and widely used \cite{robABM}. FLAME, on the other hand, was developed `in-house' at Sheffield, and so plenty of resources and information are available. Importantly, the `FLAME GPU' extension is also available to us, allowing for extremely efficient execution of models containing many agents.

\subsubsection{Swarm}
Much of this section is based on the Swarm `documentation set' book \cite{swarmDoc}.

Swarm was developed in 1994 by the Santa Fe institute as the first re-usable agent-based modelling and simulation tool, and management and development has since been taken over by the `Swarm Development Group' \cite{robABM}. Although described as being ``Probably still the most powerful and flexible simulation platform'' \cite{robABM}, every aspect of the system to be modelled must be specified in a mixture of Java or Objective-C, and it relies heavily upon the modeller having an understanding of the concepts of object-orientation. The platform is therefore not particularly accessible to the scientific community as a whole, who may not necessarily have the computer science background required to get over the initial learning-curve.

\paragraph{General concepts:}
The most important concept is that of the `Swarm'. Any swarm is composed of a collection of objects, and a `schedule of activity' which runs over those objects. The objects we place in a swarm represent everything in our simulation which we use to model with. For a simple flocking model, therefore, our swarm objects may be a collection of `boid'\footnote{Craig Reynolds' initial work on flocking using agent-based algorithms is considered seminal work in ABM, here individual generic agents which could represent fish, birds, insects, etc\ldots are termed `boids'. Further information at \url{www.red3.com/cwr/boids}} agents, a representation of the world they're interacting in, and a store of their positions. The schedule of activity describes how time influences the collection of objects. The schedule of activity for flocking, therefore, would be to tell each boid to change its position/course, and to update the object storing positions. 

Note at this point that Swarm is designed to be hierarchical; we can have swarms where one of the swarm's objects may be an entire, lower-level, swarm. Such a structure is extremely powerful and enables us to implement complex systems at various levels of abstraction. For example, we could model an ocean as a swarm composed of many schools of fish. Each school could then be its own swarm, containing individual fish agents.

Swarm simulations contain two swarms: the model swarm and the observer swarm. Combined, they contain all the code and information necessary for execution.

\paragraph{Model Swarms:}
The model swarm contains everything in the world which is involved in the simulation. All the computation relating to the actual model takes place here, so these are considered the heart of execution.
When we wish to run a simulation, we pass the model swarm parameters upon its instantiation. These parameters control the model's initial setup, including any agent distributions or any simulation-specific variables (which type of N-body algorithm to use, for example).

With reference to this project, input parameters might be the number and distribution of `star' agents, along with their masses and initial velocities. The output of a model swarm is the system's observables --- how whatever we're investigating changes with time. In our project, therefore, the outputs from execution of a model swarm would be the agents' positions in space.

\paragraph{Observer Swarms:}
We use a second swarm; the `observer swarm', to interpret the output of our model. The observer swarm contains the model swarm in its object collection. Other objects in the collection automatically pass parameters to the model swarm, and read the resultant output. The observer swarm's activity schedule handles processing of the generated data; taking results from the collection and drawing them to a graph or writing to a file, for example. 

The inputs of an observer swarm control how we process the data --- what information are we interested in and how should we deal with it. The outputs are obviously the processed results, and we have a choice between using \emph{graphics mode} (where results are displayed to the user graphically, as graphs or 2D visualisations) or \emph{batch swarms}, where many simulations are run and we store all the raw data for later statistical analysis. Importantly, Swarm does not support 3D visualisations, and so we would need to add this functionality ourselves by extending the graphics mode.

\paragraph{Swarms and OOP:}
Swarm models are created with Objective-C, although an alternate version `SwarmJava' uses a Java Objective-C library to allow the creation of models using Java with little deviation from original Swarm syntax. We can therefore create a complete Swarm model with either Objective-C, Java, or a combination of both.

Model and observer swarms are implemented as interfaces of the generic `swarm' superclass. We define their parameters to be any user input relevant to the simulation, and write our own methods for getting data or returning output as required.
Agents are interfaces of `SwarmObject' --- another generic superclass which has very little behaviour of its own. We specify all the behaviours for agent interaction in these classes, and so it is here that the majority of coding effort will have to be spent. As all agents are built as objects of their own class, we also define their variables here. The most important function is the \emph{step} method, which is called by the model schedule to increment a timestep. Here we provide the code to calculate how the agent's variables change as a result of its state and/or environment during that period of execution, and these results are updated for the next timestep.

\subsubsection{The FLAME framework}
\label{flame}
\begin{quotation}
\noindent\emph{``FLAME is a generic agent-based modelling system which can be used to development applications in many areas. It generates a complete agent-based application which can be compiled and built on the majority of computing systems ranging from laptops to HPC super computers''}\footnote{\url{www.flame.ac.uk}}
\end{quotation}

\noindent FLAME breaks the model down into a set of interlinked components; we use X-machines to model the agents' formal behaviour; XML to specify the agents' structure (including any variables), and an initial agent configuration file; and C to write the transition functions which compute agent interaction.

\paragraph{X-machines:}
We define the operation of our model with a formal model of computation known as \emph{Stream X-machines} \cite{streamX}. A stream X-machine operates as an \emph{extended finite state machine}, with the additional property that it has a memory which can be written to and read from via regular transition functions.

A full explanation of state machines is not required for the purposes of this literature review, but we will be using them to define our agents for the early stages of implementation. FLAME considers one timestep to have passed when every agent in the simulation has moved from its \emph{start} to its \emph{end} state.

\paragraph{XML:}
FLAME's models are written in XML\footnote{Extensible Markup Language; \url{http://www.w3.org/XML/}}, the particular schema for which is available at \\\url{http://flame.ac.uk/schema/xmml_v2.xsd}. By using XML for simple tasks such as the definition of agents, the system is significantly easier to manage and understand (the majority of information can be easily read by anyone who understands the XML `tag' concept), and we don't need to worry about any obscure C syntax. On the other hand, the complex core of operations --- the agent's transition functions --- \emph{are} written by us in C, allowing for far greater flexibility.

\paragraph{Input ($t_{0}$):}
\label{flamet0}
In FLAME, we define the initial conditions of our system in an XML file named `$0$.xml'. This file contains each agent's initial variables, and is read by FLAME when we first execute the simulation. This file is formatted the same as the subsequent numbered timestep files, an example for which is shown in appendix \ref{exampleT0}

Initial investigation suggests that the matter distribution of the early universe is well-represented by a 3D Gaussian distribution; if we use FLAME, one aspect of this project will be to create a small program which is capable of automatically producing an appropriate $0$.xml file.

\paragraph{Output:}
FLAME runs its simulation for a set number of $n$ timesteps, $n$ defined by the user, with one timestep concluding when every agent has moved from its start to end state. After each timestep, FLAME outputs a numbered file $x$.xml, ($x$ the iteration number) which represents all agents' variables in the same fashion as our initial $0$.xml.

Obviously, we want a more meaningful representation of the evolving system than a sequence of tens of thousands of numbered XML files. The most obvious and intuitive method of doing this is to output the results as a 3D visualisation, redrawing the positions of agents after each timestep as they move across space. Various strategies to further improve the visual spectacle of this will be available; perhaps rendering agents in a variable colour from red--blue to represent different levels of energy. 

To achieve this in FLAME a third-party program will need to be developed which takes the numbered XML files as input, parses and processes these, then draws all the information for each timestep to the screen, making use of some 3D graphics API such as openGL or Direct3D. Dr Simon Coakley, who initially developed FLAME, has recently produced the FLAMEVisualiser\footnote{\url{http://ccpforge.cse.rl.ac.uk/gf/project/flame/frs/}}; a tool written in C++ which does exactly this. FLAMEVisualiser takes FLAME output files and then draws each one as a frame in a video, representing agents with simple objects of different colours in their appropriate position in 3D space. FLAMEVisualiser provides a GUI with which to choose from various rendering options (figure \ref{visualiserConfig}), but given that this is fairly early-release software, the options for customisation aren't as extensive as we will require. It will therefore be necessary to make some adjustments to slightly tailor the video's appearance to exactly what we wish.

\begin{figure}[H]
\centering
\includegraphics[scale=0.50]{./images/flameVistest}
\caption[FLAMEVisualiser config and an example rendering]{Left: FLAMEVisualiser config controlling rendering for an example visualisation (right). Shows the rules governing which agents are drawn, as well as how they're drawn. Shape options include sphere, cube and point (spheres shown). Agent colour can be any standard RGB value, with an optional alpha channel. Also shows the current iteration (timestep) being drawn, as well as rules for automated graph production. (Author's own work, using FLAMEVisualiser's provided example model).}
\label{visualiserConfig}
\end{figure}

\subsubsection{FLAME GPU}
\begin{quotation}
\noindent \emph{``FLAME GPU is a high performance Graphics Processing Unit (GPU) extension to the FLAME framework. It provides a mapping between a formal agent specifications with C based scripting and optimised CUDA code.''}\footnote{\url{www.flamegpu.com}}
\end{quotation}
\label{flameGPU}
An extension to FLAME, FLAME GPU utilises NVIDIA's\footnote{NVIDIA design GPUs and Graphics cards; \url{www.nvidia.co.uk}} CUDA\footnote{Compute Unified Device Architecture; \url{http://www.nvidia.com/object/cuda_home_new.html}} framework for parallel processing on the GPU to provide a more efficient method of computing agent-based simulations. Having been designed in hardware to perform simple calculations at an incredible pace (the ray tracing algorithm, for example, benefits greatly from parallelisation), the GPU is built with very many processing cores. %As an example, my current desktop PC has an `Intel i7' CPU containing four physical cores clocked at $2.66$GHz each. By way of comparison, the same machine's GPU contains $240$ cores, but only runs at a combined 648MHz.

This (parallel) design fits perfectly with our specification of an ABM; a system composed of many \emph{autonomous} agents. The fact that agents act autonomously is of key importance --- if their operations don't rely on the results from previous calculations, then they can all be computed in parallel, reducing computation time potentially from $tnN$, (with $t$ the time taken to perform one agent's calculation, $n$, the number of agents and $N$ the number of timesteps), to just $tN$. Realistically this won't be the case; our simulation will contain hundreds of thousands of agents, and most consumer GPUs have between 200--500 cores.

The advantages of FLAME GPU (and GPU-based computation in general) are most substantial when we are dealing with very large numbers of agents performing simple calculations, and therefore the relative speedup over FLAME is highly dependent on the model we're simulating. Since our agents' interactions are simple mathematical operations, we can compare its likely performance well with that of the `circles' model\footnote{`Circles' is a basic force-resolution model included in FLAME GPU for benchmarking purposes.}; a graph of the performance of which over FLAME we show in figure \ref{flamePerformanceGraph}. Here we see FLAME GPU consistently performing up to 100\% faster than FLAME for population sizes of up-to $2^{16}$ on a GPU with single-precision, and up-to 300\% faster on a double-precision GPU.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.85]{./images/FlameGPUGraph.png}
  \caption[Relative speedup of FLAME GPU over FLAME]{The relative speedup of FLAME GPU over FLAME for one timestep of the `circles' model using FLAME GPU's standard `brute force' message calculation. Computed using FLAME on a single dual-core AMD Athlon 2.51GHZ, and FLAME GPU with a 9800GX2, and GTX480 \cite{emeraldGPU}}
  \label{flamePerformanceGraph}
\end{figure}

\paragraph{Output}
\label{flameGPUOutput}
Since agent information is stored on the GPU during execution, visualisation of our output is much easier for real-time rendering than it would be were we using the CPU (where data would need to first be sent to the GPU). FLAME GPU provides two methods for dealing with output, chosen by command line parameter. In \emph{Console Mode}, it behaves the same as FLAME; spitting out numbered xml timestep files, which can later be interpreted by a different program. In \emph{Visualisation Mode}, FLAME GPU draws the results of the simulation in 3D space in real-time, representing agents as spheres \cite{fgpuTechnical}. This eliminates our need to use the `FLAME Visualiser', or to create our own program. When built in visualisation mode, entire FLAME GPU models are compiled down to an executable file which takes a command-line parameter defining the location of a `0.xml' initial setup file. It is therefore possible to launch models built for FLAME GPU by executing a single batch-file, whereupon the simulation will run indefinitely, rendering itself in real-time until the window is closed. This, combined with the intuitive data representation in visualisation mode, allows simulations to be easily distributed and the results observed on different machines.

FLAME GPU does not currently allow support for multi-GPU systems such as cards in SLI\footnote{Scalable Link Interface; NVIDIA's consumer technology for connecting multiple graphics cards for single output; \url{http://www.nvidia.co.uk/object/sli_technology_overview_uk.html}}, or grid systems, although work is being undertaken to change this.

\subsubsection{GADGET-2}
\label{gadget2}
GADGET-2 (GAlaxies with Dark matter and Gas intEracT (v2), hereafter referred to just as GADGET\footnote{\url{http://www.map-garching.mpg.de/gadget/}} is a suite of code designed to execute N-body simulations by modelling particles as collisionless fluids and using smoothed-particle hydrodynamics to calculate their motion. Although not an agent-based modelling package, GADGET is included in this discussion anyway due to its relevance to the project. Because it has been designed explicitly for N-body simulations, the software has been well optimised for the task in hand, and includes a combined Tree/PM optimisation algorithm (section \ref{barnesHut}), using a particle-mesh for anything longer-range than a certain tree distance.\cite{gadgetDoc}

  GADGET also includes several different models of the universe we can use as our simulation-space. These are: 
\begin{itemize}
  \item `Newtonian' space (as in our model);
  \item `Cosmological' (expanding) space, with physical coordinates;
  \item `Cosmological' space with co-moving coordinates.
  \end{itemize} 
\noindent We can also use any of these with a selection of optimisation strategies including TreePM and periodic boundaries \cite{gadgetDoc}.

\paragraph{Output:}
GADGET, like FLAME, has been built for massively-parallel systems, and output is created as a series of `snapshot' files, conceptually similar to FLAME's ordered timestep files. Unlike FLAME however, particle data is stored in a binary, rather than ASCII, representation \cite{gadgetDoc}.

Two pieces of software are available to create visualisations of GADGET data, though their performance is unknown. These are SPLASH\footnote{A generic visualisation tool for scaled particle hydrodynamics; \url{http://users.monash.edu.au/~dprice/splash/}} and IFrIT\footnote{Ionization FRont Interactive Tool --- a generic tool for visualising any 3D data set; http://sites.google.com/site/ifrithome/}.

Both the Millennium simulation and the Bolshoi simulation (section \ref{MilSim}) used versions of the GADGET code which had been modified to improve efficiency when dealing with such vast models. GADGET basically provides a generic framework for developing N-body simulations; essentially performing the same function that FLAME and Swarm do for agent-based models.

\subsubsection{Software summary}
With a number of different software packages outlined, we need to compare and contrast their relative features to arrive at some form of conclusion. 

FLAME, FLAME GPU and Swarm are all generic, agent-based frameworks which allow us greater flexibility at the cost of a larger memory and computational overhead per particle when compared to systems such as GADGET. GADGET, by contrast, models particles as collisionless fluids and calculates their positions by way of general formulae. Beneficial features of ABM for N-body simulations such as this include the ability to model with layers of hierarchical agents written directly into our code (galaxy agents composed of star agents composed of gas agents\ldots), and that we don't have to worry about the mathematics of the large-scale system; by applying the rules of particle-interactions for agents on an individual level, the whole-system's behaviour should be \emph{emergent}. GADGET, however, has the benefit of being designed specifically for our problem domain --- cosmological N-body simulations. Such specialisation is obviously in direct contrast with FLAME and Swarm; while GADGET is highly specific, these are highly generic --- they can be used to model any system, and the level of detail achieved is determined by the time available to create an appropriate model within the framework. 

Of all our software tools, FLAME GPU is the only one which allows computation on the GPU. Currently, the software only supports processing on one device, although work is being undertaken to extend it to multi-GPU systems. Executing the project on the GPU is an extremely attractive idea; As the video-game industry drives the development of increasingly powerful GPUs at consumer-level prices, GPGPU\footnote{General-Purpose computation on Graphics Processing Units.} is rapidly being adopted as a new paradigm for scientific computing. As well as being a novel area of research, the type of calculations to be performed are perfectly-suited for the GPU, and it also allows us easier visualisation (section \ref{flameGPU}). FLAME and GADGET operate in a similar fashion; both will run on many architectures, including individual workstations as well as massively-parallel clusters, HPCs and supercomputers. Swarm, however, does not support parallel execution across distributed systems. This could pose a considerable problem given the potential time required to complete a timestep for large agent populations.

As a result of being developed in-house, we have access to significantly more documentation and direct support for FLAME and FLAME GPU than for any of the other packages. If, as is often the case when using fairly small, specific tools, the provided documentation doesn't adequately cover some obscure function-call or error message, then the software's developers are available just down the corridor! User guides for Swarm and GADGET are both available, although the Swarm guide is considerably more comprehensive.

Finally, the format of data output by the software varies between packages, and as producing a graphical visualisation of the simulation is a key objective, this is an important area of discussion. All tools have methods available for visually displaying their data, but to varying degrees of suitability. GADGET is the only tool described which does not have a first-party visualiser, and FLAME GPU is the only one which will render simulation results in real-time. Real-time rendering doesn't make a huge difference, but is helpful for debugging or improving the model --- instead of having to run the simulation for a period of downtime and then build a video from the output, we can watch problems develop as they occur, and change them then and there. We also have the option to build some bespoke visualisation software by parsing the raw output data and drawing it with some 3D graphics API in a suitable language, such as Java or C. This gives us the greatest flexibility in exchange for the significant time-investment of actually coding it. GADGET, with its binary output, is likely to prove harder to do this for than the other packages (which use ASCII), since we'll need to perform all the necessary conversations first.

\subsection{Similar investigations}
\label{similarStudies}
\subsubsection{N-body simulations}
Many projects have used supercomputers to simulate the evolution of stars, galaxies, and even universes. For the sake of brevity, I shall only outline those which are of the largest scope, and those which are the most relevant.

\paragraph{The Millennium Simulation} \label{MilSim} \cite{millenium} (also known as the Millennium Run) can, in a way, be regarded the as `poster child' of large-scale N-body simulation. Developed by the Virgo Consortium for Cosmological Supercomputer Simulations, the Millennium Run modelled with $\approx 1\times10^{10}$ dark matter particles, each with a mass of $8.6\times10^8h^{-1}M_\odot$ \footnote{$\odot$ is used to represent one solar mass, $\approx1.99\times10^{30}kg$} The project primarily modelled the evolution of the very large-scale distribution of matter, but galaxy formation obviously plays a role in this domain. 

The results from the Millennium Run were released in 2005, and images of the simulation are widely used. The full data are available for scientific use by registered users via a publicly interfaceable SQL database, or one $\frac{1}{512}$th of the data are available for immediate public use.\footnote{\url{http://gavo.mpa-garching.mpg.de/Millennium/}} Data from the Millennium simulation has been used in over 400 publications to date.\footnote{\url{http://www.mpa-garching.mpg.de/millennium/\#PUBLICATIONS}} One conclusion drawn from their initial study was that \begin{quotation}\noindent``N-body simulations of CDM Universes are now of such size and quality that realistic modelling of galaxy formation in volumes matches to modern surveys has become possible.'' \end{quotation} This ignores the computational effort required for such a simulation, however. We would hope that a smaller agent-based approach may yield similarly significant results, but only requiring a fraction of the CPU time. 

\paragraph{The Bolshoi Simulation (2011)} \cite{bolshoi} is in many respects a larger, more up-to-date version of the Millennium simulation. The WMAP Seven-year data release in 2010 \cite{sevenYWmap} further refined the cosmological parameters outlined in figure \ref{cosmologicalParameters}, finding significant differences from those values released previously and used as the basis for the Millennium simulation. Because the Bolshoi simulation is so recent, very few papers have yet been published on their findings. Their initial paper describing the simulation investigated the properties of dark matter halos in detail, but presented very little information about structure-formation.

\paragraph{GPU-based}
N-body simulations are few and far-between. In their most recent book summarising the latest research efforts on 3D graphics and GPU algorithms, NVIDIA's research team published \emph{`Fast N-Body Simulation with CUDA'} \cite{gems}. Since FLAME GPU is based on CUDA (using it to push calculations onto the GPU), any findings from this are extremely relevant to the project. 

The team implemented an all-pairs algorithm to calculate acceleration for $2^{14}$ particles entirely on the GPU. The force for a particle-particle pair $F(p_{i}, p_{j})$ was computed in serial, while the overall force for each particle was computed in parallel, so $\Sigma F(p_{i},p_{j})$ runs in a separate thread for each $i$.

The team found that:

\begin{quotation}
\noindent ``The result is an algorithm that runs more than 50 times as fast as a highly tuned serial implementation (Elsen et al. 2006) or 250 times faster than our portable C implementation.'' 

(Elsen et al's paper: \cite{elsen})
\end{quotation}

And state that:
\begin{quotation}\noindent ``It is difficult to imagine a real-world algorithm that is better suited to execution on the G80 architecture\footnote{NVIDIA's GeForce 8 series, the only GPUs commercially available which supported the CUDA framework at the time of publishing.} than the all-pairs N-body algorithm.''
\end{quotation}

\noindent These findings are particularly encouraging; to demonstrate such dramatic changes in performance using an all-pairs algorithm makes the idea of running an optimised N-body solution (such as the Barnes-Hut simulation) on the GPU an extremely inviting idea. Indeed, the team even stated that:
\begin{quotation}
\noindent ``In future work we hope to implement the Barnes-Hut or Fourier-Mesh-Method algorithms, to evaluate the savings of more-efficient algorithms.''
\end{quotation}

\noindent Finally, this study is even more relevant the project than is initially apparent. FLAME GPU includes a number of different agent-messaging protocols for computing interactions on different levels of efficiency. The most basic one of these --- the brute-force communication mode where every agent communicates with every other agent --- was developed based off the algorithms given in the NVIDIA team's publication\footnote{From personal correspondence with Paul Richmond.} (\cite{gems}). As a result, the simplest agent communication method available to us in FLAME GPU has already been designed for N-body simulation!

\subsubsection{Agent-Based Models}
The following studies were carried out with the use of a generic ABM package; either FLAME or FLAME GPU, as indicated.

\paragraph{EURACE} (EURopean Agent-based Computational Economics)
\cite{eurace} used FLAME to build a large-scale model of the European economy. Their model included up to $10^7$ agents representing \emph{households}, up to $10^5$ agents representing \emph{firms}, and up to $100$ \emph{banks}. These agents were able to learn and adapt, while those representing governments and the central bank did not; they simply followed predefined rules. A final set of agents collated and redistributed information regarding the agents (financial ratings, for example). EURACE modelled the financial interactions between each of these, and was computed in a massively parallel environment.

In the conclusions of their preliminary report, the team state that they are confident that the project will reproduce currently unexplained phenomena of real economies, and will therefore help to explain how these emerge from simple interactions. Furthermore, their results from some preliminary investigations supported this statement\cite{eurace}.

\paragraph{NARCSim}\cite{narcsim} used FLAME GPU to simulate the illegal drug market as a serious game. Agent types included drug addicts, recreational drug users, drug-dealers (optionally also users), treatment workers, non-users and police officers. The system then modelled the interactions of these agents as they went about their lives. Agents had their own variables, and their interactions with others were controlled by the interplay of these. Example variables for drug addicts would include: `timeSincelastFix', `addictionLevel', `totalMoney', etc\ldots

The model proved accurate, in that real-world trends (for example, as the population density increases, so does the ability for both drug-users and non-drug users to acquire drugs) were replicated within the simulation.

Some of the system's emergent behaviour proved particularly interesting; if a dealer doesn't have some certain type of drug requested by a user, then they may present an alternative (provided the user can afford it). If a user then takes the alternative drug some sufficient number of times, that user may become addicted to the new drug, thus mirroring the real-world concept of `gateway drugs'.

 By using FLAME GPU rather than FLAME, the team was able to run NARCSim on a single machine rather than a grid, and could provide real-time video feedback relating to the effects of any changes to the market.

\subsubsection{Summary of Previous Studies}

The Millennium Run and Bolshoi Simulation were both very large-scale studies, designed to really stretch the boundaries of our models and test their validity. Consequently, their publications focus almost entirely on the \emph{results} of the simulation, rather than its implementation. This is an important distinction to make; for these studies, the simulation is simply the \emph{tool} used to generate the results. On the other hand, NVIDIA's investigation into N-body simulation using CUDA is more of a `tech-demo' --- the working simulation and its algorithms were the \emph{results}, rather than just the tool. Here, replicating the physics in minute-detail wasn't as relevant; this was a computer science study, not a physics one. The Millennium Run and Bolshoi simulation are undoubtedly the largest cosmological N-body simulations ever created, and demonstrate the extent to which this type of simulation is possible when we have access to supercomputers.

\subsection{Literature Review Summary}
This is quite a novel area of research; very little work has been previously undertaken using agent-based frameworks within the realm of physics or cosmology. The level of accuracy of our model is essentially just a function of how much time is available --- we would be wise to start with a simple model of gravitation, and then evolve to a more complex simulation over time, ensuring that additional phenomena and mathematical constructs integrate well with existing work as we go.

We have learnt that galaxies are formed as a result of repeated collisions and mergers with smaller bodies, and can be classified into one of several groups based on their physical characteristics. This knowledge will be relevant to evaluation of our system (section \ref{evaluation})

There are various algorithms for computing particle interactions in the context of N-body simulation, with complexity ranging from O($N^2$) to O($N \log N$). Pairwise comparison is the simplest, but also the least efficient. 

Topics which may prove to be particularly problematic include incorporating relativistic effects; an expanding universe; the mechanics of star formation; the methods by which density waves propagate through the interstellar gas to form structure; angular momentum, spin and the formation of spiral arms, and the flat, disk-like nature of spiral and lenticular galaxies despite their elliptical dark matter halos.

There are various software packages available to us, and care must be taken to select the right one.
FLAME is a helpful framework for ABMs; XML makes agent definitions simple, while C allows functions to be arbitrarily complex. By compiling numbered output files to a video \emph{after} the simulation has concluded, the time taken to compute a single timestep has no effect on the video's performance (Frames Per Second). FLAME GPU is an extension to FLAME which allows it to run on the massively-parallel GPU; making complex simulations accessible to much lower-level hardware. Entire FLAME GPU models can be compiled down to a single file, then executed and visualised in real-time. This allows for easy redistribution of the model.

SWARM and GADGET are other software options. GADGET is an advanced N-body simulation tool, designed for use on grids for physics investigations. The algorithms have already been well optimised for N-body calculation, and incorporate more advanced physics than we could realistically achieve otherwise, including models with a non-zero rate of expansion of the universe, and comoving coordinates. SWARM is similar to the FLAME framework, but does not allow for parallelisation, and less documentation is available.

%Phew, literature review over!
%Onwards, to the requirements and analysis!~~~
\section{Requirements \& Analysis}
\label{Requirements}

\subsection{Project aims}
This project aims to simulate the process of galaxy formation using an agent-based model. This will involve the creation of a cosmological N-body simulation, where a large number of massive `particles' are placed in 3D space and their interactions modelled as per physical laws. 

In order to initialise the simulation, we need to define the system's starting configuration. Doing so requires specifying the number of agents of each type, as well as the starting values of any variables these agents may carry. Obviously, manually specifying the positions and velocities for large numbers of agents isn't an option. We therefore aim to develop a small program which, when given basic input parameters, generates a file containing a relevant distribution of agents ready to load into our framework. From the literature review we learnt that the conditions of the early universe are well modelled with a Gaussian random field, and so this will form the basis for one of the agent distribution algorithms.

In order to observe the evolution of our simulation, and thus determine the validity of our model, we need some method of taking output files and representing them in a more intuitive format. Another aim of the project is therefore to provide a visualisation of the system as an evolving 3D graphic. All potential software candidates outlined in section \ref{potentialSoftware} have tools to do this automatically, with varying degrees of customisability. FLAME GPU, in particular, provides a very basic but extensible visualisation shell, which we would expect to have to extend for the purposes of this project. 

\subsection{Requirements}
We here turn our project aims into a set of specific requirements.

Requirements which are \emph{`Necessary'} are deemed critical to the successful implementation of the system. Without these in place, the project is unlikely to yield any meaningful result. \emph{`Desirable'} requirements are those which are likely to significantly increase the accuracy/performance of the project, but are not essential. Finally, \emph{`Optional'} requirements are to be attempted if we are left with additional time after all necessary and desirable requirements are completed.

\begin{enumerate}
  \item Necessary
  \begin{enumerate}
    \item Our model operates in a universe of fixed, three-dimensional Euclidean space \label{n1}
    \item Our model contains some number of `particle' agents each with their own variables, at the very least including: \label{n2}
    \begin{itemize}
      \item{Position}
      \item{Velocity}
      \item{Mass}
    \end{itemize}
    \item As well as agents representing baryonic matter, we should also be able to instantiate agents as `dark' matter, the differences being: \label{n3}
    \begin{itemize}
      \item Dark matter only interacts with other particles by gravitation
      \item Visualisation of dark matter particles can be toggled on/off\label{darkVisReq}
      \item The quantity of dark matter particles is determined by the relationship between the total matter density $\Omega_{m}$ and the baryon density $\Omega_{b}$, such that $\Omega_{cdm}=$($\Omega_{m}-\Omega_{b}$) (section \ref{lcdm})
    \end{itemize}
    \item We must create some program which automatically generates a file ready for use by our choice of ABM package. At the least, must take input of: \label{n4}
    \begin{itemize}
      \item $N$; The number of `particle' agents in the system.
      \item $N_{dark} \leq N$; The number of dark matter particles.
    \end{itemize}
    \item There must exist some method for visualising simulation output with 3D graphics. At the least, this must take a set of pre-generated output files and render the resultant video when each file is drawn as a frame. \label{n5}
    \item The visualisation runs smoothly. (It has an acceptable FPS\footnote{FPS refers to the number of frames drawn per second of video. Film is shown at 24 Frames Per Second, and before this point, video can appear choppy. Our visualisation is unlikely to have a high contrast or a requirement to be perfectly smooth (unlike video games, for example), and we can therefore lower this threshold somewhat.})\label{n6}
    \item The time elapsed from simulation start should be displayed on the visualisation. This is found from (timestep number $\times$ time per timestep), and should be given in Myr\footnote{$1\times10^6$ years}. (From time scales of galaxy formation in \ref{earlyUniverse}.\label{n7})
    \item Software/Hardware\label{platformReqs}
    \begin{itemize}
      \item FLAME is platform independent can be executed on an extremely wide range of hardware, but simulations with  a large number of agents are unlikely to be able to be computed on machines with single (multi-core) CPUs (such as my own, or those in the Lewin Lab). As such, access to a grid service such as CICS's `Iceberg' cluster would be required.
      \item FLAMEGPU requires development in C in Visual Studio, and, therefore, Windows (is not platform independent).
      \item FLAMEGPU requires access to CUDA hardware, which means a single CUDA compatible NVIDIA graphics card\footnote{A list of all CUDA compatible GPUs can be found at: \url{http://developer.nvidia.com/cuda-gpus}} is required.
      \item FLAMEGPU only supports the `Double' variable type as a simulation constant in hardware with CUDA 2.0 or above. Similarly, the `Double' type can only be used as an agent variable with CUDA $\geq1.3$. My development hardware is CUDA 1.3, and so we will be restricted to global variables of the `float' type. This shouldn't pose a problem as our global constant --- the gravitational constant --- $G$$\approx6.67384\times10^{-11}$, and the smallest float is given as $\approx1.1755\times10^{-38}$
      \item SWARM is platform-independent, but intended for use with UNIX-based systems. CPU grids are not supported.
      \item GADGET is distributed for use with UNIX-based systems.
    \end{itemize}
  \end{enumerate}
  \item Desirable
  \begin{enumerate}
    \item In addition to the all-pairs algorithm, we should be able to optionally select use of the more efficient Barnes-Hut algorithm (section \ref{barnesHut}).\label{barnesReq}
    \item Our visualisation tool should have the capability to run in real-time with the simulation.
    \item When generating initial model files, we should be able to choose between different particle distributions. One such distribution should be a Gaussian random field, controlled by the cosmological parameters as given in section \ref{lcdm}. \label{d3}
    \item As well as an agent type for low-level particles, we should implement hierarchical agents. A higher-level agent which would fit well in our system would be a gas cloud composed of many individual `particle' agents.
    \item Rules for star formation given the Jean's Criterion (section \ref{jean}) should be implemented within said gas clouds.
    \item Inclusion of supernovae and/or black holes for stars above a certain mass and age threshold.
    \item Inclusion of particle collisions and/or spin (most N-body simulations treat particles as collisionless).
  \end{enumerate}
  \item{Optional}
  \begin{enumerate}
    \item Packaging all tools and executables to a single distributable application, allowing users to generate, execute and observe models specified by their own parameters.
    \item The incorporation of Relativity into the system.
    \item The use of other, more accurate, models of the universe. Particularly, modelling with an expanding universe.
  \end{enumerate}
\end{enumerate}

\subsection{Analysis}
\subsubsection{Project components}
%general
The implementation of the basic gravitational N-body simulation can be considered the core of this project. A number of the `necessary' requirements (\ref{n1}, \ref{n2}, \ref{n3}) are therefore components of this, and methods of approaching each have been discussed extensively in the literature survey. To complete the most basic N-body system, we need to design a single `particle' agent type, and create the code to compute its gravitational attraction as an agent-rule within some ABM framework. In the case of FLAME, this means specifying an agent in XML, and coding its transition functions in C (section \ref{flame}). In Swarm, we must write the entire class representing our agent and use this to form a model swarm (section \ref{potentialSoftware}). The Swarm equivalent of FLAME's transition functions is the `step' method, necessary for every agent class. In such a simple system, the only force acting upon a particle is its acceleration due to the gravitational attraction of every other particle, and so the transition/step functions need to implement this accurately.

In the basic implementation using a single agent type, the only difference between particles of dark and baryonic matter is that `dark' particles aren't shown in the visualisation of our simulation (requirement \ref{n3}). All we need to do in order to implement this in conjunction with requirement \ref{n5} is add some boolean variable `isDark' to the definition of a particle agent, and then include an GUI option on the visualisation to toggle dark matter visualisation on/off. When off, the visualiser will only draw agents with a false `isDark' flag.

%t0 creation
Our tool to create a model's initial conditions (requirement \ref{n4}) should be fairly simple. Upon execution, the application should take input controlling the number of particles, $N$, as well as a ratio of baryonic matter:dark matter. A sensible default for this ratio would be that given by the cosmological parameters found in section \ref{lcdm}. Appropriately handling other inputs such as distribution type and amplitude of matter perturbations are to be implemented as and when we move on to the `desirable' requirements (in particular, requirement \ref{d3}).
The control structure for this program is simple. In FLAME, initial files are written in XML, with a structure as shown in appendix \ref{exampleT0}. To automate production of these we therefore need some function which writes the appropriate XML for one agent given the agent's variables as parameters. We then loop on this function $N$ times, with a separate set of logic determining the appropriate variables for each agent as per our requirements (wholly random distribution, Gaussian distribution, uniform distribution, fixed masses, variable masses, no dark matter, etc\ldots). Since this tool doesn't need to run in conjunction with our modelling framework, it can be built as a standalone application. We therefore have a choice of any appropriate programming languages to write it in. Sensible options are those with good capabilities for writing to files. With this in mind, Java, C/C++ or Python are all acceptable.

%writing the visualiser
The project's third main component is the visualisation software. The extent to which we have to code this ourselves is determined by which modelling package we use, as no preexisting software suits our purposes exactly. To write the software from scratch we must create a standalone application which reads simulation output files, and parses these to find the information relevant to the simulation's status (agent positions, for example). These attributes are then drawn to the screen as a single frame for each timestep. To render the scene we need to use some 3D Graphics API, the most notable of which are Microsoft's Direct3D, and OpenGL. Requirement \ref{n6} states that our video should run smoothly. For scientific computing such as this, a frame-rate of $\geq 15$FPS (Frames Per Second) should be acceptable. If possible however, we should aim for $\geq 24$FPS.
FLAME and FLAME GPU offer the most substantial inbuilt support for 3D visualisation, and using either of these packages would only require slightly customising preexisting tools; the open-source `FLAMEVisualiser' for FLAME, or extending FLAME GPU's relevant inbuilt classes. One example of such a modification will be requirement \ref{n7}, overlaying model information onto the video.

\subsubsection{Choice of software}
We compared the selection of software available to us in detail in section \ref{potentialSoftware}. With this information, and in light of my requirements, I have chosen to use the FLAME framework --- specifically FLAME GPU --- as the modelling environment on which this project will be based. The reasons for this being:
\begin{itemize}
\item Both FLAME and FLAME GPU have been developed in-house. As such, plenty of help, information and documentation is available beyond that of the other options.
\item I am already reasonably familiar with the FLAME framework, having experimented with and attended a day's workshop on it before\footnote{The workshop, organised by Daniella Romano, contained presentations on FLAME and FLAME GPU, as well as hands-on programming exercises with FLAME GPU, 14/10/2010}.
\item Visualisation tools for FLAME (and particularly FLAME GPU) are more advanced than those available for the other software choices.
\item The FLAME framework is explicitly an agent modelling package, in contrast to GADGET. This allows us to have full-reign over the types of objects simulated.
\item The FLAME GPU visualiser runs in real-time, making model development and debugging easier.
\item By working on the GPU, the simulation should run faster (the time taken for each timestep will be reduced).
\item FLAME GPU requires a NVIDIA Graphics card with CUDA capability. My own desktop PC has a powerful CUDA compatible card for PC gaming. FLAME, on the other hand, will work on far more diverse hardware, but my machine's CPU is unlikely to perform as well as its GPU for this type of operation. In the case of FLAME, we may have been required to obtain time on one of the university's grid computing services, which would limit testing and development.
\item When demonstrating the model (for example, at the poster session), it will be possible to run in real-time rather than running a video of execution. (provided that either Lewin machines have suitable GPUs, or, more likely, that I bring my own development box.)
\item Few studies have used the GPU for N-body simulation; by doing this we're experimenting in a novel area of Computer Science.
\end{itemize}

\subsubsection{Potential Problems}
Because Agent-Based modelling works on the principle of accurately modelling low-level interaction and then observing the resulting emergent behaviour, we must be very careful about the extent to which we simplify fundamental interactions. Within the context of this project, the emergent behaviour we wish to observe is galaxy formation, and we may find that with only a basic model of gravity this simply won't occur. To help deal with this, I expect to have to spend a significant amount of time improving the model based on the observed results. Galaxy spin is an example of a property that might be particularly difficult to replicate with this approach; the galaxy spins as a whole, but on the agent-level, a particle's spin has no effect on the higher system.

Another problem is encountered in knowing how much we can reasonably simulate given system resource constraints. As the fidelity of the simulation increases (we add more agents), so does the quantity of calculations we have to perform per timestep. We must here strike an acceptable trade-off between accuracy and performance. With many agents, our simulation becomes more realistic and we can remove lossy abstractions. An extreme case; simulating a galaxy with atoms-as-agents would provide an incredibly high-fidelity model, but would clearly be computationally infeasible\footnote{A typical star contains $1\times10^{57}$ hydrogen atoms. With a typical galaxy containing $4\times10^{11}$ stars we have $1\times10^{68}$ agents to consider. Given the O$(N^2)$ complexity of pairwise N-body computation, that's $1\times10^{136}$ calculations per timestep.}. By using FLAME GPU rather than FLAME, Swarm or GADGET, we mitigate this problem somewhat by moving to the more efficient GPU. However, when visualising model results in real-time, as with FLAME GPU, we have to worry about framerates. For the video to not appear overly choppy, we would wish to render no less than around fifteen Frames Per Second. We must therefore not use a model that takes $>\frac{1}{15}$s to calculate one timestep. We can approach this problem by increasing the simulation size until the computation time is not worth the benefits from a larger simulation.

\subsection{Evaluation}
\label{evaluation}
The obvious approach to evaluate a simulation of something is to compare its accuracy against what we observe in the real world. That is, if a simulation's output replicates the actual observed phenomena, then this can be considered an accurate representation of whatever it's modelling. Since the timescales involved obviously don't allow us to observe the process of galaxy formation directly, we must instead compare our simulation against the collection of theory which explains how we \emph{believe} it to happen (the $\Lambda$-CDM). Specifically, we would hope to observe small `dwarf' galaxies forming from initial non-uniformities in the early matter distribution, and then galaxies forming as a result of collisions and mergers between dwarf galaxies. 

We can also evaluate the system's accuracy with the Hubble tuning fork (figure \ref{tuningForkDiag}). If the galaxies observed in our simulation can be classified using these groups, then this would also imply that we've done a good job of modelling the real world.

\section{Conclusions and project plan}
\label{plan}
The majority of work so far has been invested into learning the physics and other background knowledge related to the project, including those similar studies which have been previously undertaken. I have experimented with FLAME, and have a good basic understanding of the framework.

Planning specifically how to approach such a task is fairly difficult. Since implementing the basic system should be relatively easy, the majority of development time will be spent on incremental improvements, the timescales for which may be vastly different.

The plan of action is outlined below:

\begin{enumerate}
\item The first task will be to generate a formal specification for our agents using X-machines. This is particularly important, since it will have a significant bearing on how we model in FLAME GPU, and therefore the system as a whole.
\item With the theory concluded, we next do the simple task of defining our agents' XML structure, with reference to the X-machines produced earlier.
\item We now start writing the agents' structure and transition functions into FLAME GPU. This is building the `model proper' and is the main section of implementation. During this stage, we should test with a few agents initialised by hand, and with the inbuilt FLAME GPU visualisation classes.
\item The application to generate $t_0$ files should be developed in parallel with the previous step. Once the basic N-body simulation appears to work for a few agents, we can use this tool to test for many bodies.
\item Finally, we can work on adapting the visualisation software to produce a more appropriate rendering.
\item With any remaining time we should address the desirable and optional requirements with an aim to refine the simulation's accuracy

We have formed a Gantt chart of this plan, shown on the next side:
\end{enumerate}

\begin{SCfigure}
\includegraphics[scale=.85]{./images/gantt}
\caption[Project work Gantt chart]{Project Gantt chart. Task 1 particularly long because it spans the Christmas break and exam period. Tasks 3 and 5 are to be completed in parallel, and task 4 once enough of task 3 is implemented. Based on testing results we build further. Once tasks 3 and 5 are completed, we can start testing with many agents. When we are satisfied with this, we begin extending the visualiser (task 7). Once the visualiser has been suitably developed the necessary requirements are finished, so we extend the system by implementing as many desired/optional requirements as possible, testing while we go.}
\label{gantt}
\end{SCfigure}

%
%END SURVEY & ANALYSYS, BEGIN PHASE-2
%END SURVEY & ANALYSYS, BEGIN PHASE-2
%END SURVEY & ANALYSYS, BEGIN PHASE-2
%END SURVEY & ANALYSYS, BEGIN PHASE-2
%END SURVEY & ANALYSYS, BEGIN PHASE-2
%

\newpage
\section{Design}

\subsection{Design approach}
Although this can be considered more of an experimental or theoretical project than a large-scale software engineering task, it is still important to consider how we shall approach development. Various well known and documented methodologies exist and these include, but are not limited to, \emph{Agile}, \emph{Waterfall}, \emph{Top-down} and \emph{bottom-up}. From our requirements in section \ref{Requirements}, and our Gantt chart/time plan in section \ref{plan}, we have a significant amount of software engineering work to be done. This is split across three components; the main FLAME GPU functions \& XML specification, the t0 file generator, and the (potential) Visualisation extensions. With such a quantity of work and relatively little time with which to do it ($\approx$ 3 months, some of which must be occupied by work from other modules and experimentation), a fairly agile approach should be adopted. This will involve an iterative process of development, starting with the most simple system and following a design, implement, test, cycle for each component we subsequently introduce.

\subsection{Language, platform and software}
As explained in our platform requirements in section \ref{platfornReqs}, we are restricted to development in C (and Visual Studio) for the FLAME GPU simulation. C, like Java and C++, is a static, strongly typed language. Unlike Java and C++, however, C is not object oriented. This approach to typing allows for any type-errors to be identified at compile-time, and hence we trade an increase in development time for increased program stability. The principles of Object orientation are a set of constructs which I am extremely comfortable with, and it is a shame these aren't available to us. Visual Studio is Microsoft's IDE, primarily intended for development on the .NET platform (VB.NET, C\#, etc\ldots). Visual Studio was selected by Paul Richmond to be used for FLAME GPU on the back of its excellent provision for debugging CUDA applications and XML validation\cite{fgpuTechnical}. From a personal perspective, I would have preferred being able to develop in C++, Java or Python (and preferably Java), as these are the languages I have the most experience with. Similarly, I have not used Visual Studio before, and would again prefer to have been able to develop in Linux, using ViM or an appropriate IDE such as Eclipse. These restrictions shouldn't cause too much trouble, however, as C is suitably similar to C++ and other statically typed languages to be able to understand the concepts and only have to occasionally check syntax.

The initial file generator, however, must be created from scratch in a language of our choosing. It is an extremely important component of the project; without realistic instantiations, we're not going to be able to simulate anything. A significant amount of thought should therefore be invested into deciding which programming language we shall employ for this task. Given that the purpose of the application is to generate formatted XML files specifying values for each of the many variables of potentially tens of thousands of agents, it is important that our language has good functionality for handling large strings and outputting data to text files. Further requirements can be drawn from considering the nature the data to be output --- a large number of random, but related, numbers ($(x,y,z)$ positions, $(x,y,z)$ velocity vectors, and masses). As discussed in the literature review, these should follow some probability distribution. This ensures agent variables seem random on an individual basis, but when we look at many thousand together patterns emerge (In our case, higher mass densities in some regions). This observation implies the use of a programming language with good libraries for random number generation, incorporating more advanced probability functions than merely `A random float $x$ such that $A<x<B$'.

Python and Lisp are both dynamic, strongly typed languages (Python has object oriented capabilities, Lisp does not) well known for being particularly suited to efficient text processing\cite{xxyyzz fix this? bad reference? wideFinder}. Although we shouldn't need any of the more advanced features or libraries, this implies a strong capability for string manipulation. The other two considerations, Java and C++, are both static, strongly typed object-oriented languages. 

Having absolutely no experience with Lisp, it makes sense to discount this option, especially given that much of Lisp's positive attributes are shared by Python (dynamic typing, strong text processing background). Similarly, Java and C++ share much of their functionality and syntax. C++'s use of pointers allows us to write code which is potentially more efficient than Java, but for a fairly simple application such as this, that shouldn't make much of a difference. We therefore also eliminate C++ given my stronger background in Java.

Choosing between Python and Java is relatively difficult. Both are object orientated, and I have plenty of experience with each across a variety of projects. Both offer advantages; Python's dynamic typing allows for rapid development at the expense of potential instability, while Java is well suited to large projects. At this point, it is worth taking a more detailed look at the functionality available for the types of operations we will be performing in both languages. Python's `File' object has a wide selection of methods for reading from and writing to files, far beyond that of what we are likely to need (just an efficient write command). The Java FileOutputStream requires much more code to instantiate and write to ($\approx 15$ lines of Java, compared to $\approx 3$ lines of Python (see Appendix \ref{pyFileJavaFile})\footnote{This observation is true of the two languages in general; it usually takes much more code in Java to do the same task in Python (xxyyzz get ref).}. As we would expect from comparing such basic, essential libraries, their functionality is similar. If anything, the methods available to Python's File object are slightly richer than its FileOutputStream counterpart, but we would be unlikely to need any of them.

Python and Java's random number generation functions are almost identical --- both support generating pseudo random floats within a range, as well as pseudo random floats from some Gaussian distribution with specified parameters. This is ideal for our purposes of randomly distributing positions and masses.

Given that Java and Python are so functionally similar for the task in hand, choosing between them becomes much more a matter of personal choice. We settle for Python on the grounds of its ability for rapid development, an essential feature for a project with such limited time.


\subsection{FLAME GPU Simulation Component Design}

In order to describe the system's code, it is important to first understand how FLAME GPU itself is structured; much of the code is template generated based on the model description, so this section aims to make it apparent which code has been implemented by us, and which code has been generated based on that, or which is an existing part of the SDK.

We provide FLAME GPU with two files: The XML Model file, and the scripted agent functions. When these are compiled in Visual Studio using the FLAME GPU SDK, the XMLModel file is interpreted by the XSLT\footnote{Extensible Stylesheet Language Transformations --- a language used for transforming XML into other languages (in our case C).} parser, which generates the templated C files FLAMEGPU\_kernals.c, header.h, io.cu, main.cu, simulation.cu and visualisation.cu. These files, combined with the scripted agent functions file functions.c, comprise the full application framework.

The following diagram shows this relationship between each component:

\includegraphics[scale=1.0]{./images/FlameStructure}
\begin{figure}[h!]
\centering
\caption [FLAME GPU Structure]{Diagram of FLAME GPU's processing structure, from the FLAME GPU technical report\cite{fgpuTechnical}. Two of these components are provided by us, these are the XML model file (XMLModelFile.XML), and the scripted behaviour (functions.c)}
\end{figure}

\subsubsection{X-machines}
\label{xMachines}
As outlined in section \ref{flame}, the formal workings of the system are based on the concept of Stream X-machines, where software agents traverse a number of \emph{states}, by way of \emph{transition functions} (these can also read from and write to memory). Unlike FLAME, FLAME GPU does not have terminal states for its agents. Instead, we define a series of simulation \emph{layers}, where each layer is executed sequentially and contains at least one function. Where a layer contains multiple functions, they are assumed to execute in parallel. We must therefore never have functions relying upon sequential input from each other in the same layer.

The XMLModelFile is built based on our X-machines, and the templated nature of FLAME GPU means that a small change to this can result in large changes across the codebase. It is therefore important to think through our X-machines carefully, in order to reduce the number of changes that must be made later on.

Our simplest system (brute force N-body simulation), can be reduced to a single agent. This `particle' agent has variables $(x,y,z)$ position, $(x,y,z)$ velocities, and mass. This requires just one state; the `default' state, which has one transition function which loops on that state. This is the `updatePosition' function, which reads all other agents' masses and positions to calculate and update the resultant position/velocity for the agent in question. One iteration has concluded when every agent has transitioned from the `default' state to the `default' state, by way of the `updatePosition' function. This requires just one function in a single execution layer, so the whole system is defined as:
\includegraphics[scale=1.0]{./images/simpleXMachine}
\begin{figure}[h!]
\caption[Brute force N-body X-machine]{The trivial X-machine (left), and the XML fragment required for appropriate execution (right)}
\end{figure}

\noindent Because FLAME GPU requires the separation of functions requiring sequential input and output, however, `updatePosition' must be split into two functions. The first, executed once by each agent, writes a message containing that agent's mass and position to the message list. After every agent has completed this function, the message list contains one post for each agent in our system. Each agent can then execute the second function once. The second function calculates the new positions and velocities by reading each message in the list, and calculating $\frac{Gm_j(\mathbf r_j - \mathbf r_i)}{|\mathbf r_j - r_i|^3}$ for each. Each acceleration vector is summed, and we have our final result once the entire message list has been iterated. We then simply integrate this acceleration $\mathrm{d}t$ and calculate resultant velocities and positions as explained in \ref{newtonGravity}. Once every agent has performed both functions, one simulation step is over. This system is therefore:

\includegraphics[scale=1]{./images/lessSimpleXMachine}
\begin{figure}[h!]
\caption[N-body X-machine with separate I/O]{The X-machine (left), and the XML fragment required for appropriate execution (right), note that the use of execution layers makes this equivalent to the agent looping on one state with no function restrictions --- see appendix \ref{altXmachine}.}
\label{lessSimpleXMachine}
\end{figure}

\noindent Although the system above defines the simplest system that would work (and indeed, this was the version we used for around a month), for reasons of optimisation discussed in section \ref{optimisationImplementation}, we have produced a more complex, more dynamic model. This final X-machine and corresponding layer structure is:

\includegraphics[scale=.5]{./images/notSimpleXMachine}
\begin{figure}[h!]
\caption[Final, implemented X-machine]{The X-machines for each of two agents (left), and the XML fragment required for appropriate execution (right).}
\end{figure}

xxyyzz --- this isn't the case anymore!
\subsubsection{XMLModelFile.XML}
\label{xmlModelFileDesign}
These X-machines are now implemented in XML, using a schema which strictly defines the file structure. This file, `XMLModelFile.XML', is essentially the basis from which the entire simulation is built, and can generally be considered the most structurally important file. In it we first define simulation constants (known as environment variables) and the name of a C function to call to instantiate these. This allows users to define constants such as the size of the timestep $\Delta t$, at runtime.

We must also define every agent type, which includes (for each type):
\begin{itemize}
 \item{The agent's memory --- all possible variables, including their types.}
 \item{Any default values for each variable.}
 \item{Every function which that agent can perform, which includes (for each function):}
 \begin{itemize}
  \item{The function name --- as it must be called in functions.c, and the execution layer definition.}
  \item{The state the agent must be in to perform this function, as well as the state it will be in upon finishing the function.}
  \item{Any message lists the function requires access to for input, referenced by their message name.}
  \item{Any outputs the function posts, again as references to a message name.}
  \item{Whether the agent can be killed at the conclusion of the function, and if it requires random number generator access in functions.c\footnote{Due to the multithreaded nature of CUDA, functions requiring RNG access must be specified in the XML definition so that they can be appropriately allocated access to a CUDA-safe RNG library.}}
 \end{itemize}
 \item{Every state which the agent can occupy, referenced by name.}
 \item{The initial state this agent should occupy at t0.}
 \item{Whether the agent occupies discrete 2D space or continuous 3D space.}
 \item{The maximum number of agents of this type allowed in the simulation, as a value of $2^n$\footnote{This is required to set the GPU's memory buffer. There is no performance cost in setting the maximum agent number far above what actually occurs in the simulation.}}
\end{itemize}

As well as all possible agents, we must also describe all possible \emph{messages}. Messages are used for communicating input and output between agents. Because of its parallel nature, FLAME GPU utilises the concept of a `message list', where a pointer is stored to a sequence of individual messages all of the same type. Each item in a message list is the output for that message type from one agent. For each message type we must specify each variable present (by name and type), as well as a partitioning type for the inbuilt optimisation approaches, and a maximum buffer size.

We finally specify the sequence of execution in a series of layers. This contains some number of layer elements, each with at least one function which is executed at the conclusion of the previous layer's function(s).

From our literature review we know that our Particle agent must have the following variables: 
\begin{itemize} 
\item Mass (float), 
\item isDark (int)\footnote{Boolean types are not supported in FLAME GPU, so we use an int of 0 or 1}
\item x (float)\footnote{Although a name such as `xPos' might be more appropriate given that we also have x,y,z velocities, FLAME GPU's visualiser uses the values of variables named x,y,z for drawing their position in space.}\footnote{As mentioned in the platform requirements, we are restricted to use of the float type by our CUDA hardware version $<2.0$}
\item y (float)
\item z (float)
\item xVel (float)
\item yVel (float)
\item zVel (float)
\end{itemize}

The x, y and z variables are used both for visualisation as well as for calculating gravitational attraction. isDark is used only for visualisation, and mass is used only for calculating gravitational attraction. The velocity vectors are used only for calculating the new position and velocities at the end of a simulation iteration.

With the agent's variables as described above and our X-machine from figure \ref{lessSimpleXMachine}, we are in a position to write the entire content of this file. We must have one agent, with one state. This agent must contain two transition functions, with no conditions preventing their activation (other than the execution order). The first function, broadcastVars, should take no input, and must output the agents' $(x,y,z)$ positions and mass. Upon completion of this function, the agent must be in a state ready to execute the second function, updatePosition. This must take as input the message list created upon completion of the previous function, and must return the agent to its default state. One full simulation iteration has completed upon all agents executing functions 1 and 2, but under the restriction that \emph{every} agent must \emph{finish} broadcastVars before \emph{any} agent can \emph{start} updatePosition.

\subsubsection{functions.c}

The functions.c file contains the code to be executed when an agent activates a transition function. This typically involves reading some variables, performing some function on those variables, and then updating them accordingly. The function signatures are determined by how they are defined in the XML model file, that is, what inputs, outputs and agents they act upon.

All functions associated with transitions take the form: 
\begin{lstlisting}
__FLAME_GPU_FUNC__ int XMLfunctionName(xmachine_memory_agentName* agent, 
                                       otherVars) 
\end{lstlisting} Each transition has only one function. As methods of type int, we should return a 0 upon successful completion. Returning a 1 either deletes the agent (if reallocate:true is set in the XML file), or is ignored and assumed to be a 0. \emph{otherVars} represents the inputs/outputs this method can take. For each input or output we have: 

\begin{lstlisting}
xmachine_mesage_messagename_list* varname
\end{lstlisting}
\noindent (where varname takes the form messagename\_message and messagename\_messages for input and output, respectively). Because each simulation step can be executed in parallel, it is important to not directly reference or alter any agents memory as it currently is. This is why we use the message list: to output an agent's position, for example, we would write this to a message which is then appended to a message list of type position. Each message can hold many variables, as explained in section \ref{xmlModelFileDesign}. To handle output, therefore, we call 
\begin{lstlisting}
add_messagename_message(messagename_messages, variables)
\end{lstlisting}

\noindent \emph{messagename\_messages} is a pointer to the message list passed as a parameter, and so this simply adds the message containing \emph{variables} to the list of all messages of this type. Each function can only add one message per agent per message list, adding many simply results in the (most recent $-1$) being overwritten by the most recent.

Input is fairly similar. We take a pointer to the message list in the method signature, and can pull messages from this to process as we see fit. To protect against any direct-referencing across hardware or threads, individual message access is hidden behind two methods. The first such method:
\begin{lstlisting}
get_first_messagename_message(messagename_messages) 
\end{lstlisting}
\noindent provides us with a \emph{xmachine\_message\_messagename*}, whose variables can be accessed with \emph{pointer$\to$XMLvariablename}. The second method simply retrieves the next message:

\begin{lstlisting}
get_next_messagename_message(current_message, messagename_messages)
\end{lstlisting}

\noindent The main body of our updatePositions function will therefore iterate through the messagelist with some algorithm like:
\begin{algorithm}[H]
\caption{Iterating messages}
\label{messageIteration}
\begin{algorithmic}
\STATE currentMessage = get\_first\_messagename\_message(messagename\_messages)
\WHILE{currentMessage}
  \STATE Do some processing
  \STATE currentMessage=get\_next\_messagename\_message(current\_message,messagename\_messages)
\ENDWHILE
\end{algorithmic}
\end{algorithm}

\noindent In functions.c we also define any \emph{initialisation} functions, which are specified in the XML model file and called only once --- before the first simulation iteration, and before any agents are initialised or the process is threaded. These can be used to execute any code (including sequential, non-CUDA code) which we may need for the purposes of the simulation. In our case, this will be to take user input to set global simulation constants --- the size of the timestep $\Delta t$, for example. Initialisation functions are defined:

\begin{lstlisting}
__FLAME_GPU_INIT_FUNC__ void functionName()
\end{lstlisting}

\subsection{t0 generator}
As described in section \ref{Requirements}, we also require a tool to generate 0.xml files. This is responsible for turning probability distributions into x,y,z coordinates of agents, and writing these to an XML file ready to be input into FLAME GPU. With the agents' variables defined in section \ref{xmlModelFileDesign}, we know that one agent must be specified in the t0 file in XML as follows:

\includegraphics[scale=1]{./images/xagentxml}
\begin{figure}[h!]
\caption[Single Particle agent XML]{XML fragment for instantiation of a single Particle agent in a t0 file. (Assuming appropriate variables `val')}
\end{figure}

\noindent A simple such application would therefore just loop on writing these strings to a file until we have reached our desired number of particles, $N$. The variables would be controlled by some RNG with constraints passed by the user at runtime. This would only allow for one probability distribution present in our Simulation, however. 

To keep the simulation as dynamic as possible, we accept multiple particle distributions. The following set of Classes are therefore devised:
\begin{itemize}
\item Simulation --- The main class, a Simulation controls writing to a file. The Simulation opens a file for writing, writes the initial XML, and calls agent.writeAgent() for each item in a list of agents passed to it by a ParticleDistribution. When appropriate, it then writes the closing XML and safely closes the file.
\item ParticleDistribution --- A particle distribution is one group of particles which have each of their variables set by the same ProbabilityDistribution. That is to say, all agents of ParticleDistribution $x$ have the same ProbabilityDistribution governing their masses, but this may not necessarily be the same distribution which governs their velocities. (Although all velocities will use the same distribution, of course). All particles in a ParticleDistribution have the same probability of being dark matter, and all conform to a range of particleGroupIDs (see \ref{optimisationImplementation}).
\item ProbabilityDistribution --- A ProbabilityDistribution should act as a wrapper class for Python's native RNG functions. We should be able to pass ProbabilityDistributions into ParticleDistribution method calls in order to set the variables for each particle appropriately. A number of different RNG functions are required to be packaged up into this class, including random.uniform() and random.gaussian().
\item ParticleAgent --- A ParticleAgent represents one Particle. It has variables as defined in section \ref{xmlModelFileDesign}, and should be able to return a String of these ready to be formatted appropriately by a Simulation object into XML and written.
\end{itemize}

xxyyzz --- class diagram?
\subsection{real-world representation}

\section{Implementation and Testing}
Due to the nature of the FLAME framework, implementation of our model was surprisingly straightforward, and required much less coding than was expected. Indeed, I was expecting the majority of the time to be spent writing the simulation code in C, but the t0 file generator proved to require just as much work. Despite requiring much less code than I anticipated, it wasn't an easy task, however. This was due mostly to the top-down, templated approach of FLAME GPU; it was required to have a full understanding of how the simulation is going to operate \emph{before} you start writing anything. This is in stark contrast to how I usually code complex applications --- by starting off with something simple, and incrementally adding on functionality as I go. Much of the work, therefore, was in designing an accurate and robust model.

Because the two components of the project are so different, I have split this into two sections. One for that which relates to the simulation/FLAME GPU, and one for that which relates to our t0 file generator.
\subsection{Simulation}

\subsubsection{XMLModelFile.xml}

The model file is defined quite strictly by our XAgent design in section \ref{xMachines}, and so there is fairly little to say here. We defined several `environment' variables (rather than hard-coding them into the software), so that users could alter the simulation at run-time. These were the Gravitational Constant, $G$ (float); the size of the timestep, $\Delta t$ (float); the velocity damping factor, $\epsilon$ (float); the minimum interaction radius (float), and the number of particle groups (int).

This file changed very little during the implementation of our project. Some deviations from the initial design, such as the use of `debug variables' (see section  \ref{simulationTesting}), were introduced, but now that we are happy that the model performs as expected these have been removed from the final distribution. The other major changes were encountered when we introduced the optimisations detailed in section \ref{optimisationImplementation}. As mentioned previously, it was important to change this file as little as possible. Changing one agent name, for example, would result in having to change the method prototypes for each of its transition functions in functions.c

\subsubsection{Acceleration/Position calculation}
The most crucial part of the simulation code is obviously the transition function which takes agents' positions and masses, and calculates the new position for the agent in question. Because this function is executed independently for each agent, we need only define how we would update the position for a single particle in a many-particle system. As shown in algorithm \ref{messageIteration}, agents can iterate across an entire message list with a while loop. From \ref{accnFinal}, we know that calculating the acceleration relies upon performing some calculation based on each other agent's values, and then summing these. This is absolutely perfect for message iteration. Our algorithm, therefore, takes the form:

\begin{algorithm}[H]
\caption{Calculating resultant acceleration with message iteration}
\begin{algorithmic}
\STATE acceleration = ($0,0,0$)
\STATE position = agent$\to$($x,y,z$)
\STATE currentMessage = get\_first\_messagename\_message(messagename\_messages)
\WHILE{currentMessage}
  \STATE thisMass=currentMessage$\to$mass
  \STATE positionDifference = currentMessage$\to$position $-$ position
  \STATE absDifference = sqrt(positionDifference.x$^2$+positionDifference.y$^2$+positionDifference.z$^2$)
  \STATE thisAccn = (thisMass*positionDifference)/absDifference$^3$
  \STATE acceleration += thisAccn
  \STATE currentMessage=get\_next\_messagename\_message(current\_message,messagename\_messages)
\ENDWHILE
\end{algorithmic}
\end{algorithm}

\noindent At the conclusion of the while loop, the acceleration has been updated with the result of each particle in the system. We can now integrate this acceleration with those equations \ref{velocityEq}, \ref{displacementEq}, and update the position accordingly.

With our agents defined as point-masses, however, this becomes problematic. Consider a system with two equimassive particles. They will be attracted towards each other with equal and opposite acceleration, and shall collide at their midpoint. At this point, we are dividing by the absolute difference in their positions, $0$, which is undefined. Preventing particles from interacting if they occupy the same position is a simple fix to this, but leaves us with another problem. Because our formula divides by (absolute distance)$^3$, a small separation rapidly grants a huge acceleration. Now, because of our finitely small timestep $\Delta t$, particles with huge acceleration can increase their velocities substantially when approaching each other, and then `step over' the resultant deceleration when they pass through each other (due to the approximation of assuming a constant velocity between timesteps).

We have combated this problem by introducing to our equation the concept of small `damping factor', $\epsilon >0$, for the velocities\cite{gems}. This gives us a new acceleration equation:
\begin{center}
\begin{equation}
\mathbf{\ddot r_{i}}=G\sum_{i\neq j}\frac{m_{j}(\mathbf r_{j}-\mathbf r_{i})}{(|\mathbf r_{j}-\mathbf r_{i}|^2+\epsilon^2)^\frac{3}{2}}
\end{equation}
\end{center}

\noindent Given that our particles are mathematically considered points, but are drawn as spheres with a radius defined by us, it may be favourable to prevent any interaction between agents which are intersecting each other. We provide this (optional) functionality by allowing the user to input a minimum interaction radius. If the absolute distance between each point is lower than this radius, it contributes nothing to the acceleration.

The C code of our final method is therefore:
\begin{lstlisting}
xxyyzz include code listings
\end{lstlisting}

\noindent Note the use of the CUDA primitive types float3 and float4. Use of these is more efficient in a CUDA environment than using three or four native float variables.\cite{xxyyzz}

\subsubsection{Accuracy and Float Type Implications}
As mentioned in our platform requirements, use of the double type in agent functions is only supported on CUDA hardware $>2.0$; hardware which we don't have access to. We must therefore use the float type instead, which has a lower precision. This has a number of small implications on the implementation. Firstly, we must choose some small float for our $\Delta t$ value, but we must perform the operation $(\Delta t)^2$ in our integration. It is important, therefore, that we do not choose some value such that its square is below the minimum signed floating point value, (0.00001 xxyyzz CHECK THIS). This limits us, therefore, to $\Delta t\ge0.0001$. 

\subsubsection{Visualisation}
FLAME GPU includes a default visualisation, \emph{visualisation.cu}, which, like most of the framework's code, is generated at compile-time with an XSLT template. The default visualisation renders every agent of every type in every state. Agents are drawn in 3D space as red spheres of constant radius defined in \emph{visualisation.h}. Positions are determined by their x, y and z variables, if the agent type doesn't contain these then they are assumed to be 0. From our list of requirements, we must implement a number of features above and beyond this basic functionality, and so we therefore produce a custom visualisation.
\begin{center}
\includegraphics[scale=.7]{./images/flameGPURender}
\begin{figure}[H]
\caption[FLAME GPU Default rendering]{FLAME GPU Rendering of some agents using the default visualiser}
\end{figure}
\end{center}
Because this project is primarily an investigation, it was required to be quite strict about how much time to expend in extending the default visualisation. Although previous N-body simulations such as NVIDIA'S demonstration of OpenCL, and the Millennium run (xxyyzz --- cite, picture both), look quite spectacular, this `eye candy' had to be dropped in favour of functionality. We therefore used the default, templated visualisation code as a base, and made small modifications to incorporate changes we required.

We first removed drawing of all agents which aren't Particles (such as the SimulationVarsAgent detailed in our optimisation techniques). This was a relatively simple task, and just required removing the method calls and code to draw these agent types. Next, we changed the background colour from white to black, by removing the call to glClearColor() at initialisation. This small change was a vast improvement; the black background is much more appropriate for the investigation in hand. 

Next, we added the ability to toggle dark matter visualisation on/off, as stated in requirement \ref{darkVisReq}. To increase functionality with an only marginal increase in code content, we also added the ability to toggle Baryonic matter on/off, which was not part of our initial requirements. Unfortunately, the code structure of the default visualiser made this task harder. The templated code calls the draw method on each agent of type Particle, regardless of any variable's status.

One option was to make `Dark' particles a separate type of agent, and only draw that agent type when dark matter visualisation is toggled on. This would significantly increase code bloat however; we would have to replicate ALL the Particle XML code for our new dark particles (which would be exactly the same). Similarly, we would need to copy all the code for Particle transition functions, the only changes for which would be the particle type. 

Another option briefly considered would be to draw all the particles all the time, but draw those which are set to not be displayed in the same colour as the background (black). This has two major flaws. Firstly, we're wasting processing time by rendering something black with the objective of it being invisible. Secondly, and more importantly, the GPU performs depth-based hidden surface removal, where objects closer to the view pane can occlude those which are behind them. In this case, if a particle with its visualisation toggled off passed in front of a particle which had visualisation on,  the `on' particle would seem to disappear as it was occluded by other, black, particle.

Instead, we add a condition in the Particle agents' draw methods --- if the particle in question has a true isDark status, then instead of using the particle's x,y,z values to find the appropriate position, we draw it at a position outside the range of the maximum draw distance. While this might initially seem like a hack, it doesn't increase rendering time --- OpenGL uses a maximum draw distance which we can define, and so if anything is positioned further away from the viewpoint than some value, OpenGL simply skips over rendering it, not costing any computation time. To control the toggling we simply use two boolean variables with scope to the visualisation class; drawingDarkMatter and drawingBaryonicMatter. These are toggled by key press with a case statement and keyboard event handler.

Finally, it proved useful to be able to visually distinguish dark from Baryonic matter when we were drawing both. To allow this, we simply changed the ambient and diffuse terms for Baryonic matter to white, while leaving Dark matter red. Colour is passed along with position in a `float4', and is defined when we test if an agent should be drawn or not. We use non-absolute diffuse terms ($\approx 0.8$) to give the appearance of shading.

xxyyzz---Take better pictures, replace these!

\begin{figure}[H]
\includegraphics[scale=.35]{./images/ourVis1}
\end{figure}
\begin{figure}[H]
\includegraphics[scale=.35]{./images/ourVis2}
\end{figure}
\begin{figure}[H]
\includegraphics[scale=.35]{./images/ourVis3}
\caption[Our visualisation modes]{$\approx 8,000$ Particle agents with isDark=1 uniformly distributed in a cube, and $\approx 2,000$ isDark=0 agents distributed in two, densely packed spheres. From top to bottom, showing 1. Drawing of dark \& Baryonic matter, 2. Drawing just Baryonic matter, 3. Drawing just dark matter. (It is possible to draw neither, but this is, of course, just a black background)}
\end{figure}

\paragraph{Requirement \ref{n7}} states that we must display the elapsed time on the visualisation. Having used the `JOGL' OpenGL binding library for Java, I expected this task to be reasonably straightforward. Unfortunately, however, JOGL and the OpenGL C library used in the default visualisation are significantly different, and this proved to be harder than anticipated. Two of the examples included with the FLAME GPU SDK --- the pedestrians LOD and pedestrian navigation models --- included text overlays displaying simulation information in the precisely the manner in which we wished (appendix \ref{pedestrians}). It was thought that then we could reverse-engineer the technique for this from the provided code, and adapt it for our simulation. Unfortunately, both models also use complex custom visualisations, and so the techniques used could not easily be generalised to our adapted version of the default visualiser. It was known that we were required to make a 2D projection for our text, and then overlay this on the result of the 3D render, but after several days of getting nowhere we implemented a compromise. Instead of displaying just the elapsed time, we generalised this requirement to include all simulation variables: $\Delta t$, the damping factor $\epsilon$, the minimum interaction radius, and the gravitational constant, $G$. When we execute a FLAME GPU simulation it launches a console. When not in visualisation mode, this console displays the status of initialisation, as well as calculating and writing each timestep (appendix \ref{consoleAppendix}). When in visualisation mode, it just displays startup information (which GPU it's using, for example). We therefore allowed the user to generate a dump of simulation information, which is then printed to the console. Instead of displaying it superimposed on the visualisation window, the user can hit a button (`i', for [i]nformation), which triggers the appropriate write method. This can be called as frequently as we like, at any time during the simulation.

\begin{figure}[h!]
\includegraphics[scale=1]{./images/infoDump}
\caption[Simulation information dump]{An example info-dump of a simulation in progress. Elapsed time can be calculated by (real size of $\Delta t \times$Iteration number)}
\end{figure}

\subsubsection{Simulation Constants}

\subsubsection{Optimisation techniques}
\label{optimisationImplementation}
As brute-force computation is O($N^2$), we determined that one aspect of the project was to investigate different methods for optimisation, thereby increasing this number of particles we can simulate. In section \ref{optimisation} we investigated both the Barnes-Hut algorithm, and Particle-Mesh/(P$^3$M) methods. Our desirable requirement \ref{barnesReq} stated that we should attempt to implement the Barnes-Hut algorithm. Unfortunately, the nature of the FLAME GPU code made this task incredibly hard. We would have been required to develop a new data structure (the octree), for use in FLAME. This tree would have to be calculated and stored as an environment variable at the conclusion of each iteration, and then each particle's \emph{updatePosition} transition function would perform the calculation traversing the tree. Implementation of the new data structure proved too much work for a project with such a limited timespan, and so we investigated some different, simpler methods instead.

\paragraph{Our first approach} considered the idea of reducing the number of particles considered per timestep. We assign each particle a groupID, and then only calculate forces between particles of the same group. With two groups, for example, we would calculate and update the positions for all particles of group $1$ in the first timestep, and then calculate and update the positions for all those of group $2$ in the second timestep. Such an approach assumes that with many randomly distributed particles groups $1$--$k$ should be similar, and so the forces should average to be roughly the same. Obviously, because we only consider some $k$th of the particles per timestep (with $k$ the number of groups) such an approach requires O$((\frac{N}{K})^2)$ computation for one timestep. Since it now takes $k$ timesteps to update each particle, this becomes O$((k(\frac{N}{k})^2))$ = O$(\frac{N^2}{k})$ This approach worked reasonably well to increase frame rates for marginally larger $N$, but doesn't scale.

The implementation of this required the modification of our XMLModelFile; we introduced a new agent type --- simulationVarsAgent --- which keeps a record of the current iteration number. The simulationVarsAgent has just one function --- broadcastItNum --- which is activated once per iteration cycle. This function broadcasts the agent's only variable (the iteration number), and then increments it by one ready for the next iteration. We also add two additional variables to our Particle agent --- isActive and particleGroupID. At the start of each iteration we call a new Particle transition function --- setIsActive, which checks if the $(iterationNumber\equiv groupId \pmod{numGroups})$ If it is, then we set isActive for that agent to 1. When calling the broadcastVars function, we now add a restriction to only activate for agents with isActive=1. All other agents move back to their start state ready for the next iteration. The X-machine for this system can be found in appendix \ref{notSimpleXMachineAppendix}.

 The errors due to approximation were also much greater than we expected; random particle distributions rapidly `split off' into their corresponding interaction groups. (See results section)

\paragraph{A second approach} was therefore developed. Here, we again split our particles into some $k$ groups, but instead of calculating forces only between particles of the same group, we use every particle in the system. As with before, one timestep expires upon computing one group's new positions. One timestep, therefore, has complexity O($\frac{N}{k}\times N =$O($\frac{N^2}{k}$), As before. Given that the whole system has only been updated after $k$ timesteps, this is therefore O($N^2$) complexity once again. Note now that this technique has the same computational complexity as the brute force approach, but it's computing twice the number of timesteps in a given time $t$. This means that our FPS doubles; with 30FPS, each particle moves $\frac{30}{k}$ times. Although this doesn't reduce time taken to find a result, it has the effect of smoothing the visualisation. Even though most particles are still during most frames, because some are moving it has the effect of making it \emph{seem} more fluid. As with all N-body approximation methods, this comes with a trade-off for accuracy.

Implementing this was fairly straightforward given the infrastructure set up in the previous exercise. We calculate isActive as before, but this time every agent adds its position to the message list. Instead of returning to the default state if isActive=0 upon attempting broadcastVars, we run a different function based upon the agent's isActive value. Both add the agent's position to the message list, but take the agent to different states. This X-machine can be found in appendix \ref{notAtAllAppendix}.

xxyyzz --- more experimentation req' Before we can say if this is any good or not...

\subsection{t0 File Generator}

The Python scripts to generate t0 files were implemented in a fashion much more similar to that usually adaopted when developing a complex application. As first suggested in the project plan (section \ref{plan}), this was initially an extremely simple single function which was used to confirm that the gravitational attraction function appeared to be performing correctly with many agents, as well as `stress testing' how many agents we could have active in oe simulation.

Our first implementation took the form:
\begin{lstlisting}
import random
outFile=open(`./0.xml',`w')
outFile.write(`<states><itno>0</itno>')

for i in range(0, numAgents):
  outStr=`<xagent><name>Particle</name><id>'
  outStr+=str(i)
  outStr+=`</id><mass>'
  outStr+=str(mass)
  outStr+=`</mass><isDark>0</isDark><x>'
  outStr+=str(random.uniform(minX, maxX))
  outStr+=`</x><y>'
  outStr+=str(random.uniform(minY, maxY))
  outStr+=`</y><z>'
  outStr+=str(random.uniform(minZ, maxZ))
  outStr+=`</z><xVel>'
  outStr+=str(random.uniform(minXv, maxXv))
  outStr+=`</xVel><yVel>'
  outStr+=str(random.uniform(minYv, maxYv))
  outStr+=`</yVel><zVel>'
  outStr+=str(random.uniform(minZv, maxZv))
  outStr+=`</zVel></xagent>\r\n'
  outFile.write(outStr)

outFile.write(`</states>')
outFile.close()
\end{lstlisting}
(Assuming some appropriately defined variables numAgents, minX, etc\ldots)

This code is obviously rather simplistic, clunky and not particularly extendable. The use of \emph{random.uniform()} limits our distributions to cuboids, and we don't instantiate any dark matter. Once we were happy that the updatePosition function was behaving appropriately we extended and adapted this code with the classes described in our Design stage. 

\subsubsection{Simulations}
Simulation objects deal with writing data to a single file, they set the file up and close it down, as well as writing writing agent data. This class can be considered fairly boilerplate code. Although the construction of these was not strictly necessary, such a class allows us to keep everything relating to one t0 file with a single object, and makes code slightly more understandable. More importantly, it allows us to potentially use multiple Simulation objects within one script to create a number of different t0 files is one execution.

The Simulation class is defined:
\begin{lstlisting}
class Simulation:
  def  __init__(self, filename):
    self.filename=filename

  def initOutput(self):
    self.outputFile=open(self.filename, `w')
    self.outputFile.write(`<states>\r\n<itno>0</itno>\r\n')
    self.outputFile.write(`<xagent><name>simulationVarsAgent</name>
                           <iterationNum>0</iterationNum></xagent>\r\n')

  def closeOutput(self):
    self.outputFile.write('</states>')
    self.outputFile.close()

  def writeAgents(self, agentList):
    for agent in agentList:
      agentXML=agent.getAgentXML()
      self.outputFile.write(agentXML)
\end{lstlisting}

\subsubsection{ParticleAgents}
A ParticleAgent is instantiated by a ParticleDistribution, and stored in that ParticleDistribution's list of agents. It stores all the variables for one particle as defined in our XMLModel file, its only notable function is getAgentXML, which returns a formatted XML string of that particle, as it should be defined in 0.xml. As shown in the listing above, this is called by a Simulation to write the output file.

\subsubsection{ProbabilityDistributions}
The ProbabilityDistribution class acts as a wrapper for Python's native RNG functions. ProbabilityDistributions are used by ParticleDistributions when setting agents' variables (positions, for example). We instantiate a ProbabilityDistribution with a distribType, a value1, and an optional value2. The distribType parameter determines which type of probability distribution function to use --- linear, fixed, or gaussian. In the case of positions, this can also be a circle. Val1 and Val2 control the parameters of the distribution, and their context varies dependant on which distribution type we are using.

To illustrate this, a Gaussian distribution with $\mu=1$, $\sigma=0.5$, would be defined in our code:

\begin{lstlisting}
distrib=ProbabilityDistribution(`gaussian',1,0.5)
\end{lstlisting}

Similarly, if we wanted a linear (random) distribution with a minimum $1.3$ and a maximum $3.7$:
\begin{lstlisting}
distrib=ProbabilityDistribution(`linear',1.3,3.7)
\end{lstlisting}

The trivial `fixed' distribution takes only one parameter, and simply always returns this value. This is most commonly used for allocating every particle an equal mass.

The final distribution type --- `circle' --- is intended for assigning positions into circular (or spherical) distributions. In this context, val1 is a 3-tuple $(x,y,z)$ of the midpoint, and val2 is the radius, $r$.

\paragraph{The getItem function} is called to return one value according to the probability distribution. This is called by a ParticleDistribution when setting values; the code would take a form similar to:

\begin{lstlisting}
distrib=ProbabilityDistribution(`fixed',1.0)
thisAgent.setMass(distrib.getItem())
\end{lstlisting}

In the regular cases we can simply return the appropriate random (or fixed) value. In the case of the circle, however, we must return the x,y and z values in one call (since these are intradependent). When calculating a spherical distribution, we first generate a random radius $r_{a}$, with $0<r_{a}\leq r$ (with $r$ the sphere's radius). We then generate two random angles $0\leq\theta_{1}\leq2\pi$ and $0\leq\theta_{2}\leq\pi$ and calculate the resultant $x,y,z$ positions using the parametric form.

The getItem function is therefore defined:

\begin{lstlisting}
  def getItem(self):
    if(self.distribType is `gaussian'):
      return random.gauss(self.mu, self.sigma)

    elif(self.distribType is `linear'):
      return random.uniform(self.minVal, self.maxVal)

    elif(self.distribType is `fixed'):
      return self.fixedVal

    elif(self.distribType is `circle'):
      randRadius=random.uniform(0, self.radius)
      randTheta=random.uniform(0, 2*math.pi)
      randThetaTwo=random.uniform(0,math.pi)
      
      xPos=randRadius*(math.cos(randTheta)*math.sin(randThetaTwo))+self.centre[0]
      yPos=randRadius*(math.sin(randTheta)*math.sin(randThetaTwo))+self.centre[1]
      zPos=randRadius*math.cos(randThetaTwo)+self.centre[2]

      return (xPos,yPos,zPos)

\end{lstlisting}


\subsubsection{ParticleDistributions}
The ParticleDistribution class is where the majority of the work actually happens. We instantiate a ParticleDistribution with a number of agents, as well as the optional `usingZAxis', `darkMatterPercentage', and `numParticleGroups' parameters.

Fairly self-explanatory, the optional (default=True) usingZAxis parameter can be used to restrict our distribution to only two dimensions. Though is is clearly not particularly relevant to actual investigations of the universe, it can be easier to visualise the attraction between objects if there's no depth term. Such 2D simulations are often good to use when demonstrating the software, as this flat interaction is inherently easier to grasp than a 3D environment. The interactions between 3D distributions and 2D distributions has also yielded some extremely interesting simulation results, discussed in section \ref{results}.

Again, the optional darkMatterPercentage (default=0), is entirely self-explanatory. This percentage is turned into a probability, and then we generate a random number $x$, with $0\leq x \leq 1$, for each agent in the distribution. If $x\leq P(Dark)$, we set that particle to dark matter.

The optional numParticleGroups (default=1) parameter is used for our approximation approaches discussed in section \ref{optimisationImplementation}. This parameter simply controls how particles will be assigned a particleGroupID. For maximum accuracy, we distribute particles evenly between each group. With $n$ groups and $p$ particles, each group therefore has $\frac{p}{n}=q$ particles. The first $q$ particles are assigned to group 1, the second $q$ particles to group 2, and so on.

\paragraph{After instantiation} of a ParticleDistribution, we can call additional methods to set the particles' positions, velocities and masses. Each such variable (mass, for example), is governed by one ProbabilityDistribution which describes every particle's value. This is easier to explain in code; 

\begin{lstlisting}
distrib=ParticleDistribution(5000, True, 0, 1)

distribMasses=ProbabilityDistribution(`fixed', 0.5)
distrib.setMasses(distribMasses)

distribPositions=ProbabilityDistribution(`linear' -2, 2)
distrib.setPositions(distribPositions)
\end{lstlisting}

\noindent The set method assigns a value according to the distribution given for every particle in the distribution. Again, this is easier to see in code:

\begin{lstlisting}
def setMasses(self, massDistribution):
  for count in range (0, self.numAgents):
    self.particles[count].setMass(massDistribution.getItem())

  self.massesSet=True
\end{lstlisting}

\noindent setPositions is a bit more interesting. The function is overloaded; we can call it with just one ProbabilityDistribution --- in which case it uses the same function to find its $x,y,z$ positions, but we can also pass it three distributions --- one for $x$, one for $y$ and one for $z$ (except when using a circle distribution, since this implicitly defines $x,y$ and $z$ positions relative to each other). We must also return 0 for all $z$ values in the case where usingZAxis=False.

Note that setVelocities is similarly overloaded --- we can specify $x,y$ and $z$ distributions independently with three parameters, or all the same with just one.

\subsubsection{Putting it all together --- Making a t0 file}
The result of all this is an extensible set of scripts with a good range of functionality. We have considerable control over our simulation's instantiation at the expense of having to write some fairly boilerplate code when defining distributions. To make a t0 file, we write some python script which imports our relevant classes, and instantiates a Simulation which writes at least one ParticleDistribution.

This might look something like the following, which would create a large distribution of 10000 particles (of which 50\% are dark matter) randomly arranged in a 10x10x10 cube, with a central sphere of radius 1.0 containing 2000 particles.

\begin{lstlisting}
from simulation import Simulation
from probabilityDistribution import ProbabilityDistribution
from particleDistribution import ParticleDistribution
from particleAgent import ParticleAgent

if  __name__  ==  '__main__':

  #initialise simulation
  sim=Simulation('./0.xml')
  sim.initOutput()


  #First distribution - central sphere
  distrib=ParticleDistribution(2000, True, 0, 1)
  
  distribMass=ProbabilityDistribution('fixed', 0.05)
  distribPos=ProbabilityDistribution('circle', (0,0,0), 1)
  distribVels=ProbabilityDistribution('linear', -0.5, 0.5)

  distrib.setMasses(distribMass)
  distrib.setPositions(distribPos)
  distrib.setVelocities(distribVels) 

  sim.writeAgents(distrib.getParticleAgents())

  #Second distribution - cube
  distrib=ParticleDistribution(10000, True, 50, 1)

  distribMass=ProbabilityiDistribution('fixed',0.001)
  distribPos=ProbabilityDistribution('linear',-5,5)
  distribVels=ProbabilityDistribution('fixed',0)

  distrib.setMasses(distribMass)
  distrib.setPositions(distribPos)
  distrib.setVelocities(distribVels) 

  sim.writeAgents(distrib.getParticleAgents())

  sim.closeOutput()

\end{lstlisting}

\subsection{Testing}

\subsubsection{Simulation}
\label{simulationTesting}
The use of CUDA made this much harder to test than I had initially anticipated. Normally, we would debug by printing trace variables values to the console, but because agent functions are massively parallel we cannot call regular (global) functions such as \emph{printf} from within agent function code. The CUDA SDK provides a function \emph{cuprintf}, to use as a thread-safe \emph{printf}, but this doesn't integrate well with FLAME GPU. A different technique was therefore devised for debugging. We would first compile the simulation in `console' rather than than visualisation mode (described in \ref{flameGPUOutput}), which would write numbered iteration files to disk. By carefully manually specifying our 0.xml initial file, it was possible to calculate the resultant accelerations and positions for one or two agents by hand. These were then compared to our agent variables after one iteration as stated in the output file 1.xml. When they were different, we knew the formula had been implemented incorrectly (ignoring cases when the perfect value differed from that given by some tiny amount, which was attributed to rounding errors and inaccuracies with the float data type). In order to find the bug's location we introduced three new agent variables of type float --- debug1, debug2 and debug3. We wrote the values at each step of the formula to these debug variables, and then manually checked their correctness in 1.xml. When debug1 was correct but debug2 was wrong, for instance, we knew the error must be in the block of code between those two variable assignments. This process was effective, but time consuming. We thankfully had fairly little code to debug. Once the force resolution function had been demonstrated correctly, very little needed checking until we added the optimisation extensions, at which point we were required to test that the correct sets of particles were interacting with each other.

To aid in identification of which particles were which between different timesteps, we also added an addition agent variable `id', which gave each particle a unique integer ID, this has also been removed from the final version.

\subsubsection{t0 generator}

\section{Results, Discussion and Further Investigation}
\label{results}

\subsection{Performance and efficiency}
Our development hardware consisted of a stock overclocked Nvidia GeForce GTX 285. The card was reasonably high-end when bought in 2010, and can now be considered towards the upper end of mid-level. It is better than that we would expect to find in an average, new workstation, but slightly less powerful than most GPUs found in modern high end workstations or gaming PCs. NVIDIA's cards designed for HPCs --- their Quatro and Tesla series GPUs --- would perform much better, but we don't have access to these. Despite this, our GPU provides a good reference point for what simulations may be like if we were to redistribute the application to who may be interested in experimenting with the gravity resolution; individual home users, or secondary schools, for example.

To demonstrate the system's performance, the following graph plots the number of FPS (directly analogous to simulation steps per second) for a number of differently sized particle distributions. To reduce any compounding variables, however small, we use the same distribution in each case --- a cube between (0,0,0) and (2,2,2), and run the simulation in an isolated session.

xxyyzz Graph of performance of N x vs FPS y. Logarithmic???!

Interestingly, further analysis of the code reveals that the vast majority of compute time is spent simply iterating over the message list. If we run $15,000$ agents with an empty \emph{updatePosition} function, we have around 55FPS. If we now make the updatePosition function simply iterate across the entire message list, but \emph{not perform any calculations}, we drop immediately to 18FPS. Introducing the $\approx 8$ floating point operations required for calculating and updating position into the function code, we drop to 8FPS. This observation reveals a crucial bottleneck in FLAME GPU --- it is not the number-crunching power (FLOPS)\footnote{Floating Point Operations Per Second} of the GPU that is really holding us back, it's the latency associated with retrieving a message from the internal memory.

\subsection{Galaxies/cosmological}

\subsection{Further investigation}

\section{conclusions}

\newpage
\bibliography{bibliography}{}
\bibliographystyle{alpha}

\newpage
\appendix

%appendix

\section{appendices}

\subsection{Pinwheel galaxy}
\label{pinwheel}
\begin{figure}[h]
\includegraphics[scale=0.5]{./images/pinwheel}
\caption[The Pinwheel galaxy]{Image of the pinwheel galaxy showing many, tightly wound spiral arms. Taken with the Hubble space telescope. Credit NASA, ESA. From: \url{http://hubblesite.org/newscenter/archive/releases/2006/10/}}
\end{figure}

\subsection{Barnes-Hut subdivision}
\label{barnesHutImg}
\begin{figure}[h!]
\includegraphics[scale=1]{./images./barnes}
\caption[Barnes-Hut particle-space subdivision, from their initial paper.]{more detailed example of Barnes-hut subdivision of particle space, taken from their original 1986 paper \cite{barneshut}}
\end{figure}

\subsection{Flame XML structure}
\label{examplet0}
\begin{figure}[h!]
\includegraphics[scale=1]{./images/examplet0}
\caption[Example FLAME agent XML structure]{Example potential t0 file, defining the mathematical constants $\pi$ and $e$, as well as one `particle' agent and its associated variables. Also contains one `star' agent with its (different!) variables. Variables are just ideas of what agents may potentially require, and this is not indicative of the final agents' xml. Provided just to demonstrate the format. Author's own work.}
\end{figure}


\subsection{Python \& Java file output code comparison}
\label{pyFileJavaFile}
\subsubsection{Python:}
\begin{lstlisting}
outputFile=open("./0.xml", 'w')
outputFile.write("Hello World!")
outputFile.close()
\end{lstlisting}
\subsubsection{Java:}
\begin{lstlisting}
import java.io.*;
class writeFile 
{
  public static void main(String args[]){
    try{
      FileWriter outputFile = new FileWriter("./0.xml");
      BufferedWriter writer = new BufferedWriter(outputFile);
      out.write("Hello World!");
      out.close();
    }
    catch (Exception e){
      System.out.println(e.getMessage());
    }
  }
}

\end{lstlisting}

\subsection{Alternative single-state X-machine}
\label{altXmachine}
\begin{figure}[h!]
\includegraphics[scale=1]{./images/altXMachine}
\caption[Alternative Single state X-machine]{This alternative representation would also work, note that an agent looping on one function is prevented by use of the execution layers.}
\end{figure}

\subsection{Information overlay on pedestrians visualisation}
\label{pedestrians}
\begin{figure}[H]
\includegraphics[scale=.7]{./images/pedestrians}
\caption[Information overlay on pedestrians visualisation]{The text overlay used in some example FLAME GPU custom visualisations (top left)}
\end{figure}

\subsection{FLAME GPU Console}
\label{consoleAppendix}
\begin{figure}[H]
\includegraphics[scale=.7]{./images/console}
\caption[FLAME GPU Console]{FLAME GPU in Console mode. This console launches also in visualisation mode, but doesn't write timesteps to files}
\end{figure}

\subsection{Optimisation 1 X-machine}
\label{notSimpleXMachineAppendix}
\begin{figure}[H]
\includegraphics[scale=.55]{./images/notSimpleXMachine}
\caption[Optimisation 1 X-machine]{The X-machine and corresponding layer XML fragment for correct implementation of the first optimisation. Note that broadcastVars is restricted in the function definition to activate IFF isActive=0}
\end{figure}

\subsection{Optimisation 2 X-machine}
\label{notAtAllAppendix}
\begin{figure}[H]
\includegraphics[scale=.55]{./images/notAtAllSimpleXMachine}
\caption[Optimisation 2 X-machine]{The X-machine and corresponding layer XML fragment for correct implementation of the second approximation method.}
\end{figure}

\end{document}
